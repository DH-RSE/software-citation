<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="https://github.com/DH-RSE/software-citation/raw/main/schema/tei_software_annotation.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Exploring Audiovisual Corpora in the Humanities: Methods, Infrastructure, and
                    Software</title>
                <author>
                    <persName>
                        <surname>Tilton</surname>
                        <forename>Lauren</forename>
                    </persName>
                    <affiliation>University of Richmond, United States of America</affiliation>
                    <email>ltilton@richmond.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Arnold</surname>
                        <forename>Taylor Arnold</forename>
                    </persName>
                    <affiliation>University of Richmond, United States of America</affiliation>
                    <email>tarnold2@richmond.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Bergel</surname>
                        <forename>Giles</forename>
                    </persName>
                    <affiliation>Oxford University, England</affiliation>
                    <email>giles.bergel@eng.ox.ac.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Van Gorp</surname>
                        <forename>Jasmijn</forename>
                    </persName>
                    <affiliation>Universiteit Utrecht, Netherlands</affiliation>
                    <email>j.vangorp@uu.nl</email>
                </author>
                <author>
                    <persName>
                        <surname>Noordegraaf</surname>
                        <forename>Julia</forename>
                    </persName>
                    <affiliation>University of Amsterdam, Netherlands</affiliation>
                    <email>J.J.Noordegraaf@uva.nl</email>
                </author>
                <author>
                    <persName>
                        <surname>Melgar</surname>
                        <forename>Liliana</forename>
                    </persName>
                    <affiliation>University of Amsterdam, Netherlands</affiliation>
                    <email>L.M.Melgar@uva.nl</email>
                </author>
                <author>
                    <persName>
                        <surname>Williams</surname>
                        <forename>Mark</forename>
                    </persName>
                    <affiliation>Dartmouth University, United States of America</affiliation>
                    <email>Mark.J.Williams@dartmouth.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Bell</surname>
                        <forename>John</forename>
                    </persName>
                    <affiliation>Dartmouth University, United States of America</affiliation>
                    <email>John.P.Bell@dartmouth.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Ordelman</surname>
                        <forename>Roeland</forename>
                    </persName>
                    <affiliation>Netherlands Institute for Sound and Vision,
                        Netherlands</affiliation>
                    <email>rordelman@beeldengeluid.nl</email>
                </author>
                <author>
                    <persName>
                        <surname>Poell</surname>
                        <forename>Thomas</forename>
                    </persName>
                    <affiliation>University of Amsterdam, Netherlands</affiliation>
                    <email>t.poell@uva.nl</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2019-05-05T20:41:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Name, Institution</publisher>
                <address>
                    <addrLine>Street</addrLine>
                    <addrLine>City</addrLine>
                    <addrLine>Country</addrLine>
                    <addrLine>Name</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Panel / Multiple Paper Session</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>image analysis</term>
                    <term>computer vision</term>
                    <term>search</term>
                    <term>infrastructure</term>
                    <term>media studies</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>image processing</term>
                    <term>audio</term>
                    <term>video</term>
                    <term>multimedia</term>
                    <term>metadata</term>
                    <term>software design and development</term>
                    <term>English</term>
                    <term>computer science and informatics</term>
                    <term>communication and media studies</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p style="text-align:left; ">Image analysis is increasingly central to the field. DH
                scholars are a critical part of interdisciplinary teams developing infrastructure
                for analyzing audio-visual (AV) culture at large scale. The first paper titled
                "Computer Vision Software for AV Search and Discovery" describes novel approaches to
                increasing discovery of AV content in cultural heritage institutions by focusing on
                four search modalities (instance; category; identity via facial recognition; and
                text); how computer vision can complement and augment traditional metadata-based
                content management; and, use a new multimodal approach to search in which
                correspondences between the audio and visual content of videos are learnt from
                unlabelled data. “Analyzing Moving Images at Scale with the Distant Viewing Toolkit
                (DVT)” then turns to how digital humanists can use DVT and why we should be involved
                in creating image analysis infrastructure. The paper will begin with an overview of
                DVT, a software library that summarizes media objects through the automated
                detection of stylistic and content-driven metadata. The paper will then turn to how
                and why humanities scholars need to be a part of questioning and then retraining
                algorithms that address our scholarly concerns and commitments. </p>
            <p style="text-align:left; ">Training computer vision algorithms on historic images
                requires new training sets and metadata. The third paper, “ The Media Ecology
                Project’s Semantic Annotation Tool (SAT): Collaborative Synergies to Train Computer
                Vision Analysis” will discuss SAT, which enhances the precision of existing
                annotation methodologies by adding geometric targets within the frame, and also
                provides the infrastructure to instantiate an iterative process of computer vision
                algorithm refinement. The paper will detail an innovative multi-partner project to
                collate, annotate, and mobilize for algorithmic research significant curated
                metadata related to WWI footage at the U.S. National Archives. </p>
            <p style="text-align:left; "> "From 'user' to 'co-developer': Strategies for a
                User-centered Approach to Building Media Analysis Infrastructure" will then discuss
                re-centering users in the development and implementation of infrastructure. The
                research communities expected to use these platforms must be collaborators from the
                beginning so that their needs are incorporated, else broad and generalized
                infrastructure will serve few needs well. They ground their argument in a case
                study, which discusses a year-long pilot program to engage scholars in the
                development of Media Suite; a media studies toolkit for audiovisual research that is
                a part of the Dutch national research infrastructure CLARIAH. </p>
            <p style="text-align:left; ">In line with ADHO’s commitment to amplifying diverse
                voices, the panel is intentionally designed to represent different institutional,
                national, and funding structures. There are two U.S. based teams: one is from an
                elite, private Carnegie Research 2 university in the Northeast and the other is from
                a small, private liberal arts college in the South. The other two papers reflect
                work from public research-intensive universities in Europe. Funding includes
                government grants, commercial partnerships, and private philanthropy. Participants
                represent a range of fields including American Studies, Media Studies, and
                Statistics. The panel also includes three women presenters, a demographic regularly
                underrepresented in technical research, particularly computer vision. The panel is
                therefore designed to bring different perspectives on how to approach audiovisual DH
                infrastructure. </p>
            <p style="text-align:left; ">Computer Vision Software for AV Search and Discovery</p>
            <p style="text-align:left; ">This paper will describe current work in making audiovisual
                content searchable by using computer vision to identify features in images. The talk
                will, first, describe ongoing collaborations with cultural heritage sector partners
                that implement the current state of the art in computer vision software and, second,
                outline new research that improves the characterisation of AV content. The focus
                will be on VGG’s collaborations with BBC and the British Film Institute. The
                intention behind these partnerships is twofold: first, to assist maintainers of
                large audiovisual datasets to benefit from proven computer vision techniques; and
                second, to provide VGG with opportunities to refine and advance the state of the art
                in the field through working on tasks such as facial recognition across large
                datasets.</p>
            <p style="text-align:left; ">As the BBC resource is publicly accessible it will provide
                the main case-study for the first part of the paper. The BBC resource implements
                four search modalities (instance; category; identity via facial recognition; and
                text). It contains over ten thousand hours of broadcast news and factual
                programming, recorded over five years from several British TV channels, amounting to
                over 5 million keyframes. The paper will demonstrate each search mode; describe the
                methods upon which each is based; and note the performance of retrieval in relation
                to the state of the art. The paper will give an account of how the resource is
                indexed and maintained; how the collaboration is structured; and how the methods
                might be integrated into either public-facing discovery tools or in-house AV
                management systems. </p>
            <p style="text-align:left; ">Turning to the partnership with the British Film Institute,
                the paper will outline how computer vision can complement and augment traditional
                metadata-based content management.The BFI dataset combines a digital video archive
                of films and television programmes described through the use of custom metadata.
                While visual recognition and metadata-based search each have their own strengths,
                the paper will outline their strength in combination – metadata providing training
                data and ground truth, while visual recognition provides a means of identifying
                inconsistencies or errors in metadata. The paper will discuss the practicalities of
                integrating the two modes in a collection-management environment as well as the
                importance of a development ecosystem based on open source software, documented
                standards, and a support community. It will propose that the digital humanities
                community could take a leading position in building such an ecosystem for computer
                vision. Last, the paper will outline a new multimodal approach to search, in which
                correspondences between the audio and visual content of videos are learnt from
                unlabelled data, with the result of localising sound-production and improving visual
                categorisation. The paper will explore some of the potential uses of identifying
                these correspondences in digital humanities research projects. </p>
            <p style="text-align:left; ">Analyzing Moving Images at Scale with the Distant Viewing
                Toolkit (DVT) </p>
            <p style="text-align:left; ">The paper will address how digital humanists can use the
                Distant Viewing Toolkit (DVT) and why we should be involved in creating image
                analysis tools. The paper will begin with an overview of DVT—a software library that
                addresses the challenges of working with moving images by summarizing media objects
                through the automated detection of stylistic and content-driven metadata. It
                algorithmically approximates the way humans process moving images by identifying and
                tracking objects, people, sound, and dialogue. The DVT software library allows users
                to input raw media files in a variety of formats. The input files are then analyzed
                to detect the following features: (1) the dominant colors and lighting over each
                shot; (2) time codes for shot and scene breaks; (3) bounding boxes for faces and
                other common objects; (4) consistent identifiers and descriptors for scenes, faces,
                and objects over time; (5) time codes and descriptions of diegetic and non-diegetic
                sound; and (6) a transcript of the spoken dialogue. These features serve as building
                blocks for the analysis of moving images in the same way words are the foundation
                for text analysis. From these extracted elements, higher-level features such as
                camera movement, framing, blocking, and narrative style can be derived and analyzed.
                After an overview of the features, we will then discuss the output formats and how
                non-technical users explore and visualize the extracted information. This discussion
                will center on a specific application to the research question of how to organize
                and search a large collection of local television news programs.</p>
            <p style="text-align:left; ">While explaining the usage of the toolkit in addressing
                humanities questions, we also address how and why humanists should be involved in
                creating image analysis methods. We will discuss how and why humanities scholars
                need to be a part of retraining algorithms in order to build computer vision methods
                that address our concerns and intellectual commitments. Specially, we will talk
                about how we are applying transfer learning to tweak the open-source computer vision
                algorithms to better function on moving images from across the 20th century. The
                toolkit utilizes and adjusts the architecture of three open source programming
                libraries: dlib (King 2009), ffmpeg (Tomar 2006), and TensorFlow (Abadi et al.
                2016). Within these frameworks, novel computer vision and sound processing
                algorithms extract the required features. Specifically, the project draws from
                VGGFace2 for face detection (Cao et al., 2018); YOLOv3 for object detection (Redmon
                and Farhadi, 2018); AudioSet/VGGish for converting sound to text (Gemmeke, et al.,
                2017; Hershey, et al., 2017). Our work in building DVT consists of modifying and
                stitching together these six models for our specific humanities-centric needs.
                Digital humanists are well positioned to bring our critical lens to computer vision
                in order to facilitate archival discovery and visual culture and media studies
                scholarship. </p>
            <p style="text-align:left; ">Thanks to the support of a 2018 United States National
                Endowment for the Humanities Office of Digital Humanities Advancement Grant, a fully
                working development version of DVT has been made available on GitHub. The Version
                1.0 release is scheduled for Fall 2019.</p>
            <p style="text-align:left; ">The Media Ecology Project’s Semantic Annotation Tool (SAT):
                Collaborative Synergies to Train Computer Vision Analysis </p>
            <p style="text-align:left; ">This paper will introduce new work in the development of
                The Semantic Annotation Tool (SAT), an NEH-funded component of The Media Ecology
                Project (MEP) that affords the creation of precise time-base annotations of moving
                images.</p>
            <p style="text-align:left; ">MEP is working to realize a virtuous cycle of archival
                access and preservation, enabled by technological advance and new practical
                applications of digital tools and platforms. In a fundamental sense this is a
                sustainability project regarding moving image history as public memory. By fostering
                innovations in both granular close-textual analysis (traditional Arts and
                Humanities) and computational distant reading (computer vision and machine reading),
                MEP is a collaborative incubator to develop 21st-century research methods.</p>
            <p style="text-align:left; ">We will report on our innovative multi-partner
                collaboration to collate, annotate, and mobilize for algorithmic research
                significant curated metadata related to vast collections of World War I footage at
                The National Archives and Records Administration (NARA) in the U.S. The key
                collection to be focused on for this presentation is footage produced by the U.S.
                Signal Corps, which represents the first major motion picture endeavors by the U.S.
                military (1914-1936). </p>
            <p style="text-align:left; ">Within MEP's emerging research environment, Mediathread has
                allowed the documentation of written descriptions and tagging through full-frame
                time-based annotations. Deploying the Semantic Annotation Tool (SAT) enhances the
                precision of our already granular annotation methodology by adding geometric targets
                within the frame and real-time playback of annotations with sub-second
                resolution.</p>
            <p style="text-align:left; ">SAT also provides the backend infrastructure to model and
                instantiate an ongoing, iterative process of computer vision algorithm refinement
                that features 1) manual time-based annotations that help to train deep learning
                algorithms; 2) the production of many additional time-based annotations via these
                algorithms; 3) the evaluation and refinement of the new annotations by means of
                further manual annotation work within the SAT environment; and, 4) the application
                of the resultantly refined annotated dataset as the new training set. This paper
                will detail a first-draft test case of such an SAT workflow.</p>
            <p style="text-align:left; ">The collection of Signal Corps WWI footage has been
                selectively curated into a test dataset of 300 films. Within this dataset we have
                developed via manual annotation a training set of annotations from 50 films for use
                in training the Distant Viewing Toolkit, which is being developed by a team at
                another U.S. university. The training metadata is derived by translating into SAT
                the metadata from two unique collections of existing metadata: 1) the shot log of
                archival footage utilized in a recent 3-part PBS American Experience program about
                the history of WWI (the shot log generously provided to MEP by program archival
                producer Lizzy McGlynn), and 2) selected notecards of precise shot-specific metadata
                produced as part of the documentation by The Signal Corps itself (materials found
                within the NARA Signal Corps collections).</p>
            <p style="text-align:left; ">The SAT will be demonstrated to afford a unique and
                unprecedented collaboration between digital humanities scholars and scientists,
                professional (documentary) filmmakers, and cultural memory archives in the
                advancement of DH audio-visual analysis toward enhanced video search and multimedia
                information retrieval.</p>
            <p style="text-align:left; ">From “user” to “co-developer”: Strategies for a
                User-centered Approach to Building a Media Analysis Infrastructure</p>
            <p style="text-align:left; ">Digital humanists experiment with information technologies
                and develop tools, either using the scholars’ own skills or in close collaboration
                with data and system experts. Toolmaking has taken place within the digital
                humanities for over seventy years (Bradley, 2018). Thus, the design of systems and
                the understanding of their underlying processes are inherently at the core of
                scholarly investigation (van Zundert, 2017). According to Patrik Svenson’s (2010),
                this engagement assumes different forms including using information technology as a
                tool, as an object of study, as an “exploratory laboratory”, as an expressive
                medium, and as an “activist venue”. Thus, being a digital scholar implies not only
                being a “user” of a “tool” or an evaluator of its performance, but also being at the
                center of a tool’s design from the beginning. This paper describes one way in which
                scholars can directly participate in the construction of digital humanities
                infrastructure by means of a case study. We will focus on the Media Suite, a media
                studies toolkit for audiovisual research, part of the Dutch national research
                infrastructure CLARIAH .</p>
            <p style="text-align:left; ">In certain contexts, there has been a move to create
                overarching infrastructure for the sciences, humanities, and social sciences at the
                national and international level. The aim is to coordinate efforts in data sharing,
                interoperability, access, and tool provision according to standards. Thus,
                “personal”, bottom up engagement with tool building has given way to a more
                supervised, top-down way of tool and data “provision.” At the same time, these
                generalized infrastructures are expected to be usable in specific, collaborative
                research settings. The question for these infrastructures of “if we build it, will
                they come”? (van Zundert, 2012) becomes the fundamental paradox of how to attract
                scholars with specific research questions to use tools and data that is generic and
                aimed to serve broader groups. Consequently, to avoid the risk of broad digital
                humanities infrastructures creating a separation between scholars (“users”) and
                infrastructure developers (“designers”), they need to devise ways to more directly
                involve the communities that they aim to serve.</p>
            <p style="text-align:left; ">In order to do this, CLARIAH has set up a pilot scholar
                program in which scholars were invited for one year to offer feedback and help
                developing the infrastructure. This paper describes the strategies we used for
                “co-developing” the CLARIAH Media Suite with the six pilot research projects that
                were involved, including: design sessions, a living “demonstration scenario”
                document, a chat room connected to the development issues in the Github repository,
                a summer school, and the participation in a series of “demonstration” meetings
                organized by the overall CLARIAH pilot project program. In addition, this paper
                presents the results of the evaluation of the six research pilot projects. Since
                there is a lack of common evaluation frameworks (Duşa et al., 2014) in digital
                humanities projects, we evaluated the pilot projects using a questionnaire and
                interviews based on these topics categories: research outcomes, Media Suite
                improvements for research support, the scholars’ new skills, and dissemination
                outcomes (increasing their network and improving collaboration).</p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style="text-align:left; ">Arandjelović, R. and Chatfield, K., Parkhi, C.,
                        Coto, E., Zisserman, A., &amp; Vedaldi, A. (2018). “Visual Search of BBC
                        News”. VGG website. </bibl>
                    <bibl style="text-align:left; ">Arandjelovic, R., &amp; Zisserman, A. (2017).
                        Look, listen and learn. In 2017 IEEE International Conference on Computer
                        Vision (ICCV) (pp. 609-617). IEEE.</bibl>
                    <bibl style="text-align:left; ">Cao, Q., Shen, L., Xie, W., Parkhi, O. M., &amp;
                        Zisserman, A. (2018). Vggface2: A dataset for recognising faces across pose
                        and age. In Automatic Face &amp; Gesture Recognition (FG 2018), 2018 13th
                        IEEE International Conference on (pp. 67-74). IEEE.</bibl>
                    <bibl style="text-align:left; ">Coto, E. &amp; Zissermann, A. (2018). “VGG Image
                        Classification (VIC) Engine", 2017. </bibl>
                    <bibl style="text-align:left; ">Gupta, A. Vedaldi, A. &amp; Zisserman, A.
                        (2016). "Synthetic Data for Text Localisation in Natural Images", IEEE
                        Conference on Computer Vision and Pattern Recognition".</bibl>
                    <bibl style="text-align:left; ">Abadi, M., Barham, P., Chen, J., Chen, Z.,
                        Davis, A., Dean, J., (2016). Tensorflow: a system for large-scale machine
                        learning. In OSDI (Vol. 16, pp. 265-283). </bibl>
                    <bibl style="text-align:left; ">Cao, Q., Shen, L., Xie, W., Parkhi, O. M., &amp;
                        Zisserman, A. (2018, May). Vggface2: A dataset for recognising faces across
                        pose and age. In Automatic Face &amp; Gesture Recognition (FG 2018), 2018
                        13th IEEE International Conference on (pp. 67-74). IEEE.</bibl>
                    <bibl style="text-align:left; ">Gemmeke, J. F., et al., (2017). Audio set: An
                        ontology and human-labeled dataset for audio events. In Acoustics, Speech
                        and Signal Processing (ICASSP), 2017 IEEE International Conference on (pp.
                        776-780). IEEE.</bibl>
                    <bibl style="text-align:left; ">Hershey, S., et al., (2017). CNN architectures
                        for large-scale audio classification. In Acoustics, Speech and Signal
                        Processing (ICASSP), 2017 IEEE International Conference on (pp. 131-135).
                        IEEE.</bibl>
                    <bibl style="text-align:left; ">King, D. E. (2009). Dlib-ml: A machine learning
                        toolkit. Journal of Machine Learning Research, 10, 1755-1758.</bibl>
                    <bibl style="text-align:left; ">Redmon, J., &amp; Farhadi, A. (2018). YOLO-V3:
                        An incremental improvement. arXiv preprint arXiv:1804.02767.</bibl>
                    <bibl style="text-align:left; ">Tomar, S. (2006). Converting video formats with
                        FFmpeg. Linux Journal, 2006(146), 10.</bibl>
                    <bibl style="text-align:left; ">Arnold, T., Leonard, P., &amp; Tilton, L.
                        (2017). Knowledge creation through recommender systems. Digital Scholarship
                        in the Humanities, 32, ii151-ii157.</bibl>
                    <bibl style="text-align:left; ">Awad, G., Fiscus, J., Michel, M., Joy, D.,
                        Kraaij, W., Smeaton, A. F., Ordelman, R. (2016). TRECVID 2016. Evaluating
                        Video Search, Video Event Detection, Localization and Hyperlinking.</bibl>
                    <bibl style="text-align:left; ">Hamilton, K., Karahalios, K., Sandvig, C., &amp;
                        Eslami, M. (2014, April). A path to understanding the effects of algorithm
                        awareness. In CHI'14 Extended Abstracts on Human Factors in Computing
                        Systems (pp. 631-642). ACM.</bibl>
                    <bibl style="text-align:left; ">Bradley, J. (2018). Digital tools in the
                        humanities: Some fundamental provocations? Digital Scholarship in the
                        Humanities. </bibl>
                    <bibl style="text-align:left; ">Duşa, A., Nelle, D., Stock, G., &amp; Wagner, G.
                        G. (Eds.). (2014). Facing the Future : European Research Infrastructures for
                        the Humanities and Social Sciences. Berlin: Scivero Verlag. </bibl>
                    <bibl style="text-align:left; ">Svensson, P. (2010). The Landscape of Digital
                        Humanities. Digital Humanities Quarterly, 4(1).</bibl>
                    <bibl style="text-align:left; ">van Zundert, J. (2012). If You Build It, Will We
                        Come? Large Scale Digital Infrastructures as a Dead End for Digital
                        Humanities. Historical Social Research / Historische Sozialforschung, 37(3
                        (141)), 165–186.</bibl>
                    <bibl style="text-align:left; ">van Zundert, J., &amp; Haentjens Dekker, R.
                        (2017). Code, scholarship, and criticism: When is code scholarship and when
                        is it not? Digital Scholarship in the Humanities, 32, i121–i133. </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
