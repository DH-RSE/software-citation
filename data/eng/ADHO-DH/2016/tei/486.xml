<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Adding Semantics To Comics Using A Crowdsourcing Approach</title>
                <author>
                    <persName>
                        <surname>Tufis</surname>
                        <forename>Mihnea</forename>
                    </persName>
                    <affiliation>Pierre and Marie Curie University, Paris 6 (UPMC), France</affiliation>
                    <email>mihnea.tufis@gmail.com</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2016-03-05T13:44:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Maciej Eder, Pedagogical University in Krakow</publisher>
                <publisher>Jan Rybicki, Jagiellonian University</publisher>
                <address>
                    <addrLine>Institute of Polish Studies</addrLine>
                    <addrLine>Pedagogical University</addrLine>
                    <addrLine>ul. Podchorazych 2</addrLine>
                    <addrLine>30-084 Krakow, Poland</addrLine>
                    <addrLine>maciej.eder@ijp-pan.krakow.pl</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.21">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>comics</term>
                    <term>annotation</term>
                    <term>crowdsourcing</term>
                    <term>data quality</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>audio, video, multimedia</term>
                    <term>encoding - theory and practice</term>
                    <term>information retrieval</term>
                    <term>data modeling and architecture including hypothesis-driven modeling</term>
                    <term>xml</term>
                    <term>crowdsourcing</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <div type="div1" rend="DH-Heading1">
                <head>Introduction</head>
                <p>With over 85 million print units sold in 2014 for the top 300 comic book titles only, the comics industry is reaching a new high for the first time since 2007 (before the economical crisis). And this doesn’t include the increasingly popular graphic novels or the increasingly more accessible digital comics. The resurgence of comics and the establishment of the graphic novel as a literary genre prompted Humanities scholars to turn their attention on comics as a medium. In this paper, we address the difficulty of creating a digitized corpus by using a crowdsourced approach for annotating comic books. The resulting XML-based encodings could assist not only researchers, but publishers and collection curators equally.</p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Motivation</head>
                <p>Our approach should provide Digital Humanities (DH) scholars with a (currently missing) structured, annotated corpus; this should enable or speed up research related to the comics and sequential art theory: identifying the rhythm of the narration based on the shape, size or number of panels and its relation to the depicted action, investigations about the style of comics authors, historical periods, cultural movements etc.</p>
                <p>Curators and collectors (professional or amateur) of physical or online comics collections would be provided with a structured content which could be more easily integrated within their collections or databases. This may assist them into enlarging public or private databases of characters or comics series and enable the creation of artefacts such as comic books dictionaries, search indices and dictionaries of onomatopoeia. A certain number of projects are already in place and could greatly benefit from the creation of comic books annotations. We mention here the Grand Comics Database
                    <note place="foot" xml:id="ftn1" n="1">
                        <p rend="footnote text"> http://www.comics.org/</p>
                    </note> (an online database of printed comics), Comic Book Database
                    <note place="foot" xml:id="ftn2" n="2">
                        <p rend="footnote text"> http://comicbookdb.com/</p>
                    </note>, Digital Comics Museum
                    <note place="foot" xml:id="ftn3" n="3">
                        <p rend="footnote text"> http://digitalcomicmuseum.com/</p>
                    </note> (a collection of scanned public domain comics from the Golden Age) or the Catalogue of the 
                    <hi rend="italic">Cité Internationale de la Bande Dessinée et de l’Image</hi>
                    <note place="foot" xml:id="ftn4" n="4">
                        <p rend="footnote text"> http://www.citebd.org/</p>
                    </note> from Angoulême.
                </p>
                <p>From a publishing perspective, current standard specifications related to digital comics, such as EPUB’s Region Based Navigation (Conboy et al., 2014) and Metadata Structural Vocabulary for Comics (Ichikawa et al., 2014) are taking care exclusively of the presentation layer (i.e. rendering a publication on a screen device). But the artistic nature of comics and the great potential digital comics have already showcased allow us to go beyond simple content presentation. We believe that the data we are collecting will allow publishers and digital comics authors to create better, enhanced content and in the end a superior reading experience.</p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Crowdsourcing Annotations for Comics</head>
                <p>Participants to our crowdsourcing experiment (Azavea, 2014; Sharma, 2010) are digital comics readers. Previous research has identified expertise sharing, belonging to a community and helping with a research project as strong motivating factors for crowdsourcing participants (Dunn et al., 2012). In addition, our industrial partner will incentivize participants with product vouchers for their platform
                    <note place="foot" xml:id="ftn5" n="5">
                        <p rend="footnote text"> Actialuna – Sequencity: https://www.sequencity.com</p>
                    </note>.
                </p>
                <p>The tasks we propose are organized around a set of questions regarding a series of comics-related topics (Eisner, 1985; Ichikawa et al., 2014) on which computer algorithms are not performing well enough: complex page layouts, identification of narration elements (characters, places, events, objects), stylistic elements (balloon shapes, onomatopoeia, movement lines).</p>
                <figure>
                    <graphic url="486/image1.png" rend="inline"/>
                    <head>Figure 1. The 4 key annotation themes</head>
                </figure>
                <p>We aggregate the answers (Feng et al., 2014; Snow et al., 2008) taking into account the reliability of an annotator in a given context (task difficulty, user experience with the task, type of question) and the agreement between the annotators (Nowak et al., 2010). A quality score is thus generated for each annotation, with the best of them being selected as solutions.</p>
                <p>We subsequently are able to generate the ComicsML encodings (Walsh, 2012). This XML derived format is particularly useful since it’s based on the already widespread Text Encoding Initiative (TEI), allowing for declarations of page structure and composition, panels, characters, text (in all the varieties hosted by the comics medium: different types of balloons, diegetic text, onomatopoeia), events and even panel-to-panel transitions.</p>
              <div type="div2" rend="DH-Heading2">
                <head>Page structure</head>
                <p>The annotators are presented with a simple interface (Fig. 2) in which they will have to make a choice between a set of suggested grid layouts. These layouts are the output of applying the automatic frame extraction algorithm developed by (Rigaud et al., 2011). Alternatively, for complex page layouts, they will be asked to draw the page layout themselves.</p>
              </div>
              <div type="div2" rend="DH-Heading2">
                <head>Character identification</head>
                <p>At this step, we ask the “crowd” to simply enumerate all the characters they can identify in the current page. Characters are identified by reading their names in the text, recognising them from experience or simply giving a general statement about the character (e.g., “
                    <hi rend="italic">masked man</hi>” may be referring to 
                    <hi rend="italic">Batman</hi>). Using state of the art symbolic learning algorithms we fusion generic and specific information (e.g. if 
                    <hi rend="italic">Batman</hi> and “<hi rend="italic">masked man</hi>” both have high quality scores in the same context, they will both denote the same concept and will be considered as valid annotations).
                </p>
              </div>
              <div type="div2" rend="DH-Heading2">
                <head>Places identification</head>
                <p>We ask the annotators to simply enumerate all the places they can recognise on the current page. We are particularly looking for named places (e.g., 
                    <hi rend="italic">Gotham City, NY, planet Mars</hi>), but will also ask the annotator to mark any generic place that he might consider important for the scenes in the page (e.g., “
                    <hi rend="italic">the interior of a bank</hi>“ [in case of a robbery], “<hi rend="italic">inside a space ship</hi>” [in case of a battle in space]). These are exactly the kind of very specific annotation tasks for which state of the art image recognition algorithms are expected to fail.
                </p>
              </div>
              <div type="div2" rend="DH-Heading2">
                <head>Events identification</head>
                <p>This is yet another highly specific recognition task. Annotators are asked to describe the most important events occurring in the page. The solutions generated at this step, together with the annotations obtained in the previous steps will be used to further build the ComicsML encoding of the page.</p>
                <p>Comics scholar Scott McCloud stresses the role of ellipsis (“the blood in the gutters“ – the space between two panels) as an artistic mean for authors to engage their readers, and describes a typology of these transition spaces (McCloud, 1993). ComicsML allows us to declare such transitions through the #ana attribute of the cbml:panel element, giving us the possibility to investigate their use and their distribution across different cultural spaces (France/Belgium, Japan/Korea, USA).</p>
                <figure>
                        <graphic url="486/image2.png" rend="inline"/>
                        <graphic url="486/image3.png" rend="inline"/>
                        <head>Figure 2. Annotation interface (drawing page structure-left, narration elements-right)</head>
                </figure>
              </div>
              <div type="div2" rend="DH-Heading2">
                <head>Non-visual cues</head>
                <p>Comics are a special medium, making use of the visual to depict all other non-visual senses, with the help of different drawing “tricks”, such as:</p>
                <list type="unordered">
                    <item>Smoke coming out of a cigarette may engage the reader’s smelling sense</item>
                    <item>Onomatopoeia form a particular language of their own; comics and especially manga authors have proven great creativity when it comes to expressing different sounds via stylised text (e.g., “
                        <hi rend="italic">POW!</hi>” – punch, “
                        <hi rend="italic">BAM!</hi>” – gunshot)
                    </item>
                    <item>Horizontal lines around a car suggest the car is moving at high speed, while around a ball, they express the ball’s movement</item>
                </list>
                <p>Researchers could study, for instance, the drawing style of an author and his use of non-visual cues, and go as far as creating onomatopoeia dictionaries for comics (to our knowledge, such dictionaries already exist for manga, but not for American nor European comics).</p>
                <p>At the end of this stage, we should be able to generate a reasonably complete ComicsML encoding of the current page (see Fig. 3).</p>
                <figure>
                    <graphic url="486/image4.png" rend="inline"/>
                    <head>Figure 3. A fragment of the ComicsML encoding for the page presented above</head>
                </figure>
              </div>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Conclusions</head>
                <p>We have presented the outline of our crowdsourced annotation system for comics, as well as details of how we have designed our tasks, having in mind three main aspects: the limits of current digital comic book formats, the specifications behind the ComicsML metadata schema and theoretical principles of comics as a medium (Eisner, 1985). Last, we have presented the way in which the collected results are merged into the final ComicsML encoding and have briefly discussed some potential applications.</p>
            </div>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend="bold">Azavea, SciStarter.</hi> (2014). 
                        <hi rend="italic">Citizen Science Data Factory, Part 1: Data Collection Platform Evaluation</hi>.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Dunn, S. and Hedges, M.</hi> (2012). Engaging the Crowd with Humanities Research. 
                        <hi rend="italic">Crowd-Sourcing Scoping Study</hi>. Centre for e-Research, Dept. of Digital Humanities – King’s College, London.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Conboy, G., Duga, B., Gardeur, H., Kanai, T., Kopp, M., Kroupa, B., Lester, J., Garrish, M., Murata, M. and O’Connor E.</hi> (2014). 
                        <hi rend="italic">EPUB Region Based Navigation 1.0</hi>. 
                        <ref target="http://www.idpf.org/epub/renditions/region-nav/">http://www.idpf.org/epub/renditions/region-nav/</ref> (accessed 5 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Eisner, W. (1985).</hi>
                        <hi rend="italic">Comics and Sequential Art: Principles and Practices From the Legendary Cartoonist</hi>. Norton&amp;Company, USA&amp;U.K.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Feng, D., Sveva, B. and Zajac, R. (2009).</hi> Acquiring High Quality Non-Expert Knowledge from On-demand Workforce. 
                        <hi rend="italic">People’s Web Meets NLP-2009</hi>, ACL (2009), 51-56.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Ichikawa, D., Kasdorf, B, Kopp, M. and Kroupa, B.</hi> (2014). 
                        <hi rend="italic">EPUB Region Based Navigation Markup Guide 1.0</hi>. 
                        <ref target="http://www.idpf.org/epub/guides/region-nav-markup/">http://www.idpf.org/epub/guides/region-nav-markup/</ref> (accessed 5 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend="bold">McCloud, S.</hi> (1993). 
                        <hi rend="italic">Understanding Comics – The Invisible Art</hi>, Harper Collins, USA.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Nowak, S. and Ruger, S.</hi> (2010) How reliable are annotations via crowdsourcing? A study about inter-annotator agreement for multi-label image annotation. In 
                        <hi rend="italic">Proc. MIR-2010</hi>, ACM, 557-566.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Rigaud, C., Tsopze, N., Burie, J.-C. and Ogier, J.-M.</hi> (2011). Robust text and frame extraction from comic books. 
                        <hi rend="italic">GREC-2011</hi>, Springer, 129-138.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Sharma, A.</hi> (2010). Crowdsourcing Critical Success Factor Model: Strategies to harness the collective intelligence of the crowd. Working paper.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Snow, R., O’Connor, B., Jurafsky, D. and Ng, A.Y.</hi> (2008). Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. 
                        <hi rend="italic">EMNLP-2008</hi>, ACM, 254-263.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Walsh, J.A.</hi> (2012). Comic Book Markup Language: an Introduction and Rationale. 
                        <hi rend="italic">DHQ-6</hi>, 1. 
                        <ref target="http://www.digitalhumanities.org/dhq/vol/6/1/000117/000117.html">http://www.digitalhumanities.org/dhq/vol/6/1/000117/000117.html</ref> (accessed 5 March 2016).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
