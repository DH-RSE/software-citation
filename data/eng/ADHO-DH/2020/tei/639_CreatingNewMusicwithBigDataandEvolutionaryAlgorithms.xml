<?xml version="1.0" encoding="UTF-8"?><?xml-model href="https://github.com/DH-RSE/software-citation/raw/main/schema/tei_software_annotation.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title type="full"><title type="main">Creating New Music with Big Data and Evolutionary Algorithms</title><title type="sub"/></title></titleStmt><author><persName><surname>Lupker</surname><forename>Jeffrey A T</forename></persName><affiliation>The University of Western Ontario</affiliation><email>jlupker@uwo.ca</email></author><author><persName><surname>Turkel</surname><forename>William J</forename></persName><affiliation>The University of Western Ontario</affiliation></author><editionStmt><edition><date>43985</date></edition></editionStmt><publicationStmt><publisher>Name, Institution</publisher><address><addrLine>Street</addrLine><addrLine>City</addrLine><addrLine>Country</addrLine><addrLine>Name</addrLine></address></publicationStmt><sourceDesc><p>Converted from an OASIS Open Document</p></sourceDesc></fileDesc><encodingDesc><appInfo><application ident="DHCONVALIDATOR" version="1.22"><label>DHConvalidator</label></application></appInfo></encodingDesc><profileDesc><textClass><keywords scheme="ConfTool" n="category"><term>Paper</term></keywords><keywords scheme="ConfTool" n="subcategory"><term>Lightning</term></keywords><keywords scheme="ConfTool" n="keywords"><term>Music; Big Data; Artificial Intelligence; Musical Composition; Music Theory</term></keywords><keywords scheme="ConfTool" n="topics"><term>Europe</term><term>English</term><term>North America</term><term>Contemporary</term><term>artificial intelligence and machine learning</term><term>music and sound digitization, encoding, and analysis</term><term>Humanities computing</term><term>Musicology</term></keywords></textClass></profileDesc></teiHeader><text><body><p>Creating New Music with Big Data and Evolutionary Algorithms </p><p>Jeffrey A. T. Lupker &amp; Dr. William J. Turkel </p><p>The availability of large musical datasets like the Million Song Dataset (Bertin-Mahieux et al 2011) has made it possible for researchers in Music Information Retrieval (MIR) to develop artificial intelligence (AI) techniques that perform a wide variety of musically-based tasks. In part, this is because algorithmically-determined characteristics of a song, including its key, mode, pitch, tempo and timbre, can be used to create labelled instances for training supervised learners like various kinds of neural nets. At the same time, the existence of these large datasets has made it possible for researchers to take a &#8216;big data&#8217; approach to various styles of western music. One notable example is the work by Serra? et al (2012) which showed the changes and trends related to pitch transitions, the homogenization of the timbral palette and growing loudness levels that have shaped pop music over the past 60 years. The authors went on to suggest that past songs might be modernized into new hits by restricting pitch transitions, reducing timbral variety and making them louder. </p><p>We believe that it is possible to make use of musical big data to develop new kinds of compositional tools that employ AI to aid in the creation of new music. Although there is certainly a tradition of using computers in composition, many tools that have been created without a grounding in musical theory have often failed to attract the attention of composers. Existing examples of AI that have been implemented in music composition have largely been directed toward autonomously generated music and not toward tools that might aid a composer (Briot et al., 2019). </p><p>Here we present work-in-progress that attempts to bridge the gap between the big data approach, which is based on algorithmically determined physical properties of a song, and an approach grounded in strong musical theory. From the perspective of the music theorist and composer, a number of techniques that are common in the MIR literature do not always make much musical sense. We have been able to improve performance of mode and key detection algorithms, for example, simply by incorporating more musical knowledge into them. Our long-term goal is to determine how a series of AI algorithms trained to learn from pitch and timbre relationships according to mood, might aid and benefit a composer in the process of creating new music. What can the study of pitch transitions and timbral relationships offer the creation of new mood-based music compositions? </p><p>The system that we will discuss uses a genetic algorithm trained on pitch and timbre relationships from a large dataset. Given a desired mood, the program makes suggestions on musical structuring, chord progressions, timbre options based on chord selections and even melodic segment suggestions given in real-time, akin to the &#8220;autocorrect&#8221; feature on smartphones. As each suggestion is offered by the program, the composer can select whether or not they like it, dislike it or feel neutral towards it, so the program can continue to learn from the composer with each iteration of the genetic algorithm. </p><p>References </p><p>Bertin-Mahieux, Thierry, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. &#8220;The Million Song Dataset.&#8221; In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011), 2011. http://millionsongdataset.com </p><p>Briot, Jean-Pierre , Gae?tan Hadjeres, and Franc?ois-David Pachet. &#8220;Deep Learning Techniques for Music Generation: A Survey.&#8221; arXiv:1709.01620 (7 Aug 2019) https://arxiv.org/abs/1709.01620 </p><p>Serra?, Joan, A?lvaro Corral, Maria?n Bogun?a?, Marti?n Haro, and Josep Ll. Arcos. &#8220;Measuring the Evolution of Contemporary Western Popular Music.&#8221; Nature Scientific Reports 521 (26 July 2012). https://www.nature.com/articles/srep00521 </p></body></text></TEI>