<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yoann/Work/Grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.4-SNAPSHOT" ident="GROBID" when="2019-06-05T06:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer-Assisted Conceptual Analysis of Textual Data as Applied to Philosophical Corpuses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Guy</forename><surname>Meunier</surname></persName>
							<email>meunier.jg@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chartrand</surname></persName>
							<email>lochartrand@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Valette</surname></persName>
							<email>mvalette@inalco.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
							<email>jcheung@cs.mcgill.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Guy</forename><surname>Meunier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chartrand</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université du Québec à Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">National Institute for Oriental Languages &amp; Civilizations</orgName>
								<orgName type="institution">Université du Québec à Montréal</orgName>
								<address>
									<country>Canada, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Marie-Noëlle Bayle</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Université du Québec à Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer-Assisted Conceptual Analysis of Textual Data as Applied to Philosophical Corpuses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Overview</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many DH projects call upon computer tools for descriptive and analytic purposes: lexicon statistics, concordances , parsers, descriptive statistics, classifications , topic modeling, annotations, automatic summa-rization, visualization tools, etc. They have been mainly applied to literary, political, journalistic, or otherwise mediatic corpuses. However, less work has been done on philosophical corpuses, or corpuses that have been tailored for the ends of philosophical investigation. Computational approaches specialized for highly theoretical and abstract texts have proved their efficiency in various domains, particularly in those pertaining to the encoding, curation and presentation of textual data (e.g. digitization, web publishing, authorship analyses, etc.). These successes open avenues for more complex tools and methods to study linguistic features which are not directly observable, such as narratives, themes and concepts. Concepts, in particular, constitute a key issue, given, on one hand, the polysemy of the concept of concept, and, on the other hand, its widespread use in philosophy and other disciplines. A conceptual analysis is a process through which we decompose and thus elucidate the meaning of a concept. While it is traditionally practiced from the armchair, concepts&apos; meanings are reflected in the texts where they are expressed. As such, the development of methods and approaches to computer-assisted conceptual analysis of texts (CACAT) has the potential to make conceptual analysis more precise, more reliable, more exhaustive and more inclusive. These new challenges call for an appropriation of modern computational tools which have demonstrated their potential at discovering such entities for natural language processing. The last two decades have seen important developments in computational linguistics and in machine learning which have enabled researchers to detect and manipulate various complex features of textual data. Complex objects such as entities, events, topics, arguments, or syntactic and discourse relations can now be detected and studied. Progress in modelling and learning approaches from fields like probabilistic modeling and neural networks have made it possible to represent complex representations between various latent and explicit textual features , and to learn them in efficient ways. Because they capture different aspects of concepts, including those which appear to be latent or implicit, the innovations could open new and exciting horizons for conceptual analysis. In order to exploit this potential, digital humanists must participate in the conception of the aforemen-tioned innovations. The humanities have problems and conceptual tools of their own, which differ from those of computer scientists, and, as such, ought to be enunciated and translated into tasks for algorithms to fulfill. On the other hand, computational approaches to conceptual analysis pose specific problems, which must be addressed as new methods are developed: in-determinacy of the nature of a concept and of the method of analysis, complexity in the relation between and among concepts, diversity of interpretations, in-certitude in the evaluation schemes, limited set of computation tools to explore conceptual structures, shallowness of visualization tools, etc. These difficulties call for work from social scientists and humanists.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modelling Computer assisted conceptual analysis in text (CACAT)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jean-Guy Meunier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conceptual analysis paradigms</head><p>In many fields of scientific research, be they social sciences, natural sciences or even professional practices, abstract or highly theoretical concepts are explored to discover their content and deepen the knowledge they embed. However, there is no consensus on the nature of a concept or on the methodology to analyze them. For example, how would one proceed in analyzing the concept of Evolution in Darwin's writings? Three radically different paradigms parameterize the methodology: philosophical, linguistic and cognitive.</p><p>In the philosophical paradigm, concepts are identified to the meaning of predicative words. For some, their analysis aims at finding the conditions (necessary, sufficient, fuzzy, etc.) under which these words refer to objects, events or actions in a possible or actual world. For others, analysis consists mainly of identifying the sense or intention of these words as related to the epistemic or metaphysical conditions for their understanding. Finally, for some others, an analysis should consider the use and context (linguistic, social or other) of these words. Hence, in this philosophical paradigm, conceptual analysis becomes a sort of logico-pragmatic analysis of the meaning of words. In <ref type="table">our Darwin example, this paradigm would therefore  ask what are the meaning conditions of the word evo- lution when Darwin uses it.</ref> In the linguistic paradigm, concepts are also related to the meaning of words. For the Saussurian structuralists a concept is the core meaning embedded in the structure of the signified (le signifié)signifié) of words. For the neo-structuralists, the generativists and the cognitivists, a concept is also equated to the semantic content of predicative linguistic expression, and meaning is understood as a complex set of semantic properties (features, relations, frames, nets, etc.,) underlying isolated words or their position in sentences and discourse. Here, conceptual analysis becomes identified with classical semantic analysis of words. In Darwin's works, the analysis would explore the semantics properties of the English word evolution: for instance, it would study its lexical content, its synonyms, its topics, is semantic nets, etc.</p><p>In the cognitive paradigm, concepts are the results of cognitive or mental operations. For psychology, they are seen as a sort of cognitive categorization. For the analytical and hermeneutic traditions of philosophy of mind, they are mental states or world representations. Conceptual analysis consists then in exploring how semiotic or linguistic forms embed categories, intentions, conceptual spaces, beliefs, mental states, Weltanschauung, etc. Hence conceptual analysis bears resemblance to an exploration of cognitive operations or states: representing, categorizing, reasoning, argumenting, entailing, etc. In our analysis of Darwin, this cognitive paradigm would focus the analysis on the mental operations underlying the meaning of evolution. How is this category of mental representation acquired, built reasoned on, argued, etc.?</p><p>Choosing a paradigmatic methodology for analyzing concept is difficult, then, because not one of them is canonical. Conceptual analysis becomes an even more acute problem when computations are introduced in the methodology. The level of complexity of the task is so high that is not obvious how a computer assisted conceptual analysis of text (CACAT) project can be realized. Should it be computer-tool-driven or model-driven?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tool driven approaches</head><p>The first type of approach is tool-driven. Once a methodology inspired by one of the paradigms presented above is chosen, its practitioners use some computer programs already built and inserts them in appropriate moments of the analysis procedure. Many computer tools for this task actually exist.</p><p>A first set of tools focuses on the lexical expressions of a concept. The most classical ones are concordancers, collocation and lexical analysers, taggers, etc. These tools explore the lexical properties and contexts of one or a few canonical predicates, expressing the specific concept to be analyzed. The limits of these types of tools lie in their underlying design hypothesis: a conceptual content is to be explored through specific canonical expressions. Such a hypothesis restricts the exploration of the conceptual content to one or a specific number of predicates. This is problematic, for as we know, concepts can be expressed in language in a myriad of ways. For example, it would be very problematic to restrict Darwin's concept of evolution to the analysis of the word evolution alone. Secondly, they may produce results that are larger than the original text. This is the case of the concordance of the concept of Esse in the Thomas Indexicus. Finally, sometimes, the opposite happens. These tools may deliver only a fraction of the overall textual segments or word collocations whose content is pertinent. For instance, in Darwin only uses a few dozen times the lexical form evolution. Hence concordance, collocation, etc. on such a small sample are not very fruitful for a conceptual analysis.</p><p>A second set of tools highly influenced by classical AI approaches focuses on natural language processing (NLP). These tools are sensitive to various meaning aspects of words, such as their semantic definition, their encyclopedic, pragmatic discursive content, etc. They promise to deliver finer results for a conceptual analysis. But these tools also have limits. Their underlying hypothesis is that these semantic, pragmatic and encyclopedic information added in the grammar and the lexicon will enhance the exploration of conceptual content. Unfortunately, the added information has often been collected from common and ordinary semantic knowledge of shared language usages. Such tools will then often tend to identify already known properties belonging to this common information about the lexical conceptual word under inquiry. And most of the time, it will ignore the properties that precisely are the one that are specific to the concepts analyzed mainly when they are original, and belong to a reflexive, creative literary or reflexive discursive process, etc. These semantic properties would not be part of the common doxastic conceptual content. For instance, a philosophy scholar using such types of tools would not be very satisfied in discovering that Darwin's concept of evolution is a name meaning an action of the type change and applied to the object: natural species.</p><p>Recently, a last set of tools that are more mathematically grounded, such as neural net and Bayesian classification, vector semantics, machine learning, deep learning, etc., have become appealing and are used in language processing, They can process large data and learn semantic information by themselves. But like the other set of tools they have their limits. First, they are nor readily usable. They are in fact very complex algorithms, and are not easily mastered by humanities scholars. Secondly, their lack of traceability becomes a major obstacle when applied to large and theoretical textual data where results become difficult to evaluate. Thirdly, they seem more successful for information retrieval applications than for digging into deep conceptual content. For the moment, we are not sure that how they can effectively assist conceptual analysis.</p><p>From these remarks, it does not seem to us that conceptual analysis can only be a serendipity tooldriven approach. The results produced by these tools have not yet convinced the scholarly community that practices expert conceptual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model driven approaches</head><p>The second type of approach is model-driven. Recent philosophers of science such as Morgan and Morrison (1999), Giere, (1999), <ref type="bibr" target="#b16">Leonelli (2007)</ref> for instance, see science as a building models process where models are heuristic means for describing, explaining and understanding reality. And Mc Carthy (1999), has seen this modelling approach as a means of better understanding digital humanities interpretative projects. For our part, we explore this hypothesis and see CACAT as type of scientific inquiry where various models are used as intermediaries for understanding the analysis of highly theoretical and abstract concepts. In this perspective, we distinguish four types of models: conceptual, formal, computational and experimental.</p><p>A conceptual model defines parameters for identifying, explaining and understanding the properties and structure of linguistic items expressing conceptual content. A formal model translates certain aspects of a conceptual model in some controlled formal language that describes or identifies properties and relations of these conceptual expressions. A computational model translates some formal expressions of the formal model into algorithms and programs. Finally, an experimental model designs implementation of these formal models in a concrete computer where the analysis can be simulated and ultimately evaluated in correspondence to the other models.</p><p>In a concrete procedure, all these models interact and can be modified and adjusted. This allows the inquiry to be controllable and repeatable. It has been our own experience that, if a computer assisted conceptual analysis project is to be successful it must construct at least these four models. A CACAT project cannot bypass these models and their interactions.</p><p>Designing these models, their interactions and their experimentation to see CACAT as a scientific endeavour and not just computer gadget exploration. But each model is not built easily. And nothing comes smoothly. They are part of the research process. And much work must be done to clarify the conceptual, formal, computational and experimental models pertinent for a successful and pertinent conceptual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digital epistemology for concept analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathieu Valette</head><p>In the humanities, theory is most of the time outlined with texts: papers, books, conference presentations, lectures etc. we claim that the scientist is first a reader and a text producer. This textuality is so ordinary that it is almost invisible, and, as such, not considered as an object of science. Moreover, theories are read as synchronic systems, or even achronic systems, depending on their specific purposes (describing one fact, explaining one phenomenon...). Scientists appropriate models and concepts like tools; they have to know their function and how to manipulate them, but they do not care about knowing practical details of their enunciation. In fact, they ignore them, more or less. They find such details embarrassing, because they make concept borders fuzzy: lexicons, glossaries, and also handbooks, as they extract the concepts from their context, and standardise the definitions, creating an illusion of stability and tangibility. But concept textuality necessarily has an incidence, not only on interpretation, but also on theorisation. If the scientist is a text producer, then theorisation is the construction of meaning. Theorisation is forced by enunciation, and scientific works, beyond their materiality, can be considered as text.</p><p>The textual aspect of scientific works had been noticed by those in Europe looking at epistemological culture. In this respect, French philosopher Michel Foucault's works, in the 1960s, must be acknowledged (see e.g. <ref type="bibr" target="#b13">Foucault 1969</ref>). Foucault put in place a philological analysis of discourse centred on the combination and evolution on specific discursive structures. His purpose is, firstly, to recognize "discursive formations", i.e. stabilized relations, regularities between objects, types of speech act, concepts and topics; and, secondly, to recognise breakpoints in idea system his-</p><note type="other">tory. Foucault followed the example of some of his famous predecessors, such as Gaston Bachelard, Georges Canguilhem and Martial Gueroult. Bachelard's notion of Epistemological break, or Canguilhem's notion of concept shifts shows, for instance, that the history of a concept is not that of its increasing rationality and refinement, but that of the different fields in which they have been designed and validated. What we will call digital epistemology is a linguistic approach to this style of French epistemology.</note><p>Our topic is the study of scientific texts using, on the one hand, corpus linguistics tools which have been developed over the 40 last years and, on the other hand, a linguistic methodology (see <ref type="bibr">Rastier 2009</ref><ref type="bibr" target="#b23">, Valette 2003</ref>. Thus, our purpose is to develop tools and methodology Foucault did not have, among other reasons because some textual phenomena-as, for example, lexicon evolution, which depends on the reader's subjectivity-are invisible to a classical philological analysis. Concept emergence, concepts' individual and inter-related evolutions, the appropriation of a specific thematic, palinode, etc. constitute further examples. We do not adopt the logician's position, considering that conceptualization is a linguistic phenomenon with its own construction rules linked to a particular function of language. Neither do we ignore the psychological, social and interactional reasons of the development of concepts. Firstly, we consider that textualityi.e. the constraints of the textual layout, formulations, be they constraints of syntactic, semantic, lexical or related discursive traditions (including genres and speech)-plays a major role in concept formation. Secondly, we do not consider texts only as resources to mine and extract terminological and conceptual material, but as archives, or, in other word, as the objective tracks of the process of creating concepts.</p><p>In essence, we focus here on concept emergence considered as the result of a slow and gradual stabilisation of contextual semantic feature. Drawing on recent critical readings of Saussure's semiology (see Rastier 2015), we propose to consider a concept as a stabilized semantic form; that is, as a combination of semantic features (or semes) mainly inherited from various contexts in which it has occurred. Eventually, we link concept design with text production rather than identification of items in a general ontology <ref type="bibr" target="#b24">(Valette 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic models for conceptual analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Louis Chartrand</head><p>The last two decades have seen the rise of topic models in natural language processing (NLP). From the early successes of Latent Semantic Analysis, which decomposes datasets into "conceptual" dimensions, the introduction of probabilistic and generative models have enabled the discovery of underlying structures that condition the lexicon of a text. Those structures, in turn, are used to construct meaningful representations of corpuses and documents, and have proven fruitful in improving performances in many NLP tasks.</p><p>Those tools have interesting potential for the Digital Humanities, as they discover entities which are, on one hand, robust features of textual data, and, on the other hand, easily representable and interpretable by humans. For instance, topics may help in tasks such as categorizing documents or selecting a relevant subcorpus for analysis. However, once topics are represented using the words to which they are likely to be associated, they can also be used to make sense of what a set of textual segments are about, or to visualize the evolution of discourse in a corpus through time. As such, topics have interesting potential when it comes to representing textual data and improving our analyses of it.</p><p>In this presentation, some prominent topics models-LSA, LDA and DTM-will be presented and contrasted, and their potential uses for Digital Humanities will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Semantic Analysis (LSA)</head><p>Introduced by <ref type="bibr" target="#b12">Deerwester et al. (1990)</ref>, LSA used tools from linear algebra, in particular singular vector decomposition, to transform the representation of text segments in the form of word counts to a representation in the form of participation to "concepts", or semantic dimensions.</p><p>As words give us a good idea of what a text is about, it is common practice in text mining to represent text segments by counting its words. A document containing "apple", "orange" and "pear" a high number of times each is likely to talk about fruits. And if multiple documents share the same words, they are likely to share common topics. However, this approach does not fare well with synonyms, which it does not recognize.</p><p>The LSA uses co-occurrence in word uses to synthesize the word-count representations into more compact semantic dimensions. As synonyms tend to have the same cooccurents, they also tend to participate to the same semantic dimensions. As a result, the new representation is closer to a semantic representation than was the word-count representation, hence the name "Latent Semantic Analysis".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Dirichlet Allocation (LDA)</head><p>While LSA still is part of every NLP reputable toolset, it falls short in at least two key aspects. Firstly, its semantic dimensions are hard to read for a human: from a list of its most prominent words, it is usually hard to give a satisfying interpretation of what a semantic dimension is about ( <ref type="bibr" target="#b9">Chang et al. 2009</ref>). Secondly, it has no clear hypothesis as to how text is structured. On one hand, this makes it harder to explain semantic dimensions in terms of linguistics, psychology or discourse analysis. On the other, it means that LSA gets only part of the picture, and better algorithms with additional assumptions might produce better semantic dimensions.</p><p>Latent Dirichlet Allocation ( <ref type="bibr" target="#b5">Blei et al. 2003</ref>) is a probabilistic models which attempts to address this latter issue, and ends up addressing the former as well. It supposes that in a corpus, there is a certain number of topics, which, when activated, make it more or less likely for specific words to be present. Thus, when someone writes a document, LDA assumes that she selects a certain restricted number of topics, which in term condition which words will be found in the document. Using this assumption and an arbitrary number of topics, the algorithm infers the most likely list of topics, and their most likely assignments to documents.</p><p>As such, it produces once again a representation of documents or text segments from word counts, but in terms of topics rather than more abstract conceptual dimensions. The words most likely to be present when a given topic is activated are often visibly related, either semantically or because they participate in a transparent narrative. As such, they are easily read by a human interpreter, and can be used to give a sense of what documents, sub-corpuses or textual segments are about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Topic Models (DTM)</head><p>Another boon of LDA is that its probabilistic model can be modified account for particularities of the corpus, or to model features that we want to study in particular. For example, if we have a corpus that spans across decades, we might expect topics to evolve with time, as society and culture change.</p><p>To model this, the algorithm devised by <ref type="bibr">Blei &amp; Laf- ferty (2006)</ref> uses a corpus split in time slices (say, per year) and topics are split accordingly, such that topic 1 at time 1 is different from topic 1 at time 2. Then, a Markov assumption is enacted on the time series: topic 1 at time 1 conditions topic 1 at time 2, which conditions topic 1 at time 3, etc. This gives topics the freedom to evolve, while enforcing a certain degree of conservatism.</p><p>Using this, one can not only track topics more efficiently, but also see the evolution of topics across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is a topic?</head><p>What is it, however, that we are talking about when we speak of LSA's conceptual dimensions or LDA's topics? Can it be equated with the notion of TOPIC that we encounter in discourse analysis, for instance?</p><p>While there are a variety of definitions for words such as "topic" and "theme", most agree that a topic is what a text is about <ref type="bibr">(Rimmon-Kenan, 1995)</ref>. On this score, LDA's topic does seem to agree with the common notion of TOPIC: a word list representing a LDA topic is read as a representation of what the textual data is about <ref type="bibr" target="#b3">(Blei, 2013)</ref>. Furthermore, the information the probabilistic model captures is the one that is redundant across a number of text segments. As such, it highlights words and concepts which keep coming back as they put in relation with various entities in sentences. In other words, textual discourse and narratives are being sewn around them.</p><p>On the other hand, humans tend to make slightly different representations of topics compared to machines (Chang 2010), more readily constructing topics around concepts and thus providing sparser (more compact) representations. As Chang suggests, this might be because humans build these representations using general domain knowledge, whereas topic models try to infer this knowledge from word distributions. This seems to tell us that we should understand LDA topics as indicators, or reconstructed traces, of the topics that underlie a text, but not as true representation of topics themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using topic models in conceptual analysis</head><p>As Chang's 2010 experiment suggest, topics entertain special relation with concepts, as a topic tends to be associated with a restricted number of concepts which are expressed very often in the text.</p><p>As such, topic models' potential in representing textual data can be exploited to discover associations that are likely to be useful to conceptual analysis and other philosophical analyses. For instance, it can help the analyst identify the most important parts of a corpus, and those that can be discarded. They can also be leveraged to build representations of the contexts in which the concept of interest appears, thus giving a sense of the topics with which it is associated. Using DTM, one can also get a sense of the evolution of a concept within a diachronic corpus. Beyond discovery of new informations concerning a concept's expression in a corpus, topic models can be useful to test some association, as the structures they uncover are relatively robust.</p><p>That said, as the DTM model shows, topic models can be used in a large variety of use cases, as their model can be expanded to take into account a corpus' metadata and thus open new and innovative avenues for conceptual analysis and the Digital Humanities in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised natural language processing for conceptual analysis of events</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jackie Chi Kit Cheung</head><p>In unsupervised machine learning, an algorithm is trained to discover regularities in data without access to human-provided labels. Such techniques can be useful in conceptual analysis of text, in cases where we do not have or want to impose a schema on the text corpus under analysis. The basic intuition behind unsupervised natural language processing techniques is that objects that appear in similar contexts in the data should be assigned similar representations, such that they can be grouped into clusters.</p><p>Unsupervised models differ according to several characteristics, from the type of information that is made available to the learner, to how similarity is defined between the different objects that are modeled, to the expected form of the output cluster that is learned. For example, the Latent Dirichlet Allocation (LDA) topic model ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) is a probabilistic model which is given access to multiple documents for training. The crucial assumptions behind the LDA model are that each document can be described as a mixture of multiple topics, and each word in a document is generated by one of the topics in that mixture. As a result of training an LDA model, multiple topics are learned, which correspond to clusters of words that tend to co-occur in the same documents.</p><p>More recently, there have been a number of unsupervised models that have been used to discover the structure of a sequence of entities and events that appear according to some narrative in the natural language processing literature <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b7">Cheung and Penn, 2013</ref>). This is accomplished by explicitly modelling the sequential dependencies of events as they appear in a document. I will provide an overview of the assumptions of the event structure being learned by such models. For example, some methods produce discrete sequences of prototypical event and participant roles. In the work of Chambers and Jurafsky, (2008), narrative chains are learned corresponding to prototypical roles in a narrative. A chain such as _ accused X; X claimed _; X argued _; _ dismissed X might correspond to a defendant in a trial. Other work frame the problem as a task for probabilistic learning. <ref type="bibr" target="#b7">Cheung and Penn (2013)</ref> define a probabilistic sequence model, in which the structure of an event and its participants are explicitly represented in the model as latent random variables. The nature of a learned cluster, then, would be how it influences the conditional probabilities of generating other cluster labels, as well as the word emission distributions from that latent topic (as in an LDA model).</p><p>I will discuss how such models can be used to discover templates of prototypical events, including how events and event participants are typically expressed in language. Such approaches can easily be applied to multiple domains, including texts in the legal or medical genres, because they make minimal assumptions about the structure of events, and do not require training data. I also discuss other applications of these models to information ordering, and automatic summarization, which may be of interest to researchers in conceptual analysis for the digital humanities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A computer-assisted analysis of SYMPTOM in psychiatry</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marie-Noëlle Bayle</head><p>The Diagnostic and Statistical Manual of Mental Disorders (DSM-5) is a general classification and diagnostic tool used in the rich and diverse universe of mental health. Being widely distributed and available online, it allows everyone to have a direct access on how to make a psychiatric diagnosis. To facilitate its reading, laypeople and professionals alike may consult definitions for important notions in the glossary section. However, these few lines will often fail to capture the complexity of a term. For instance, at the core of the clinical assessment of a disorder lie its signs and symptoms. Therefore, a proper understanding of what a symptom means, and how this concept relates to the disorder, is essential to the diagnostic approach.</p><p>In the DSM-5 glossary, symptom is defined as "A subjective manifestation of a pathological condition."</p><p>In the practice, it is often treated as a necessary and/or sufficient condition for to seek diagnosis, or as a constraint on possible diagnoses. As such, non-doctors and patients often think of a symptom as a singular event, and conceive of it purely in terms of content and a means to a diagnosis.</p><p>However, as experience of mental disorders is often messier, we may wonder if such a notion of SYMPTOM overlooks important features of the way this notion is actually reflected in the DSM-5. Our hypothesis is that both the glossary definition for this concept and the common understanding of this notion fail to account for such essential aspects. To provide evidence for this claim, we performed a computer-assisted conceptual analysis of text (CACAT) for the concept SYMPTOM in the DSM-5. We find evidence that SYMPTOM is strongly associated to a dimension of temporality, and that it is expressed in relation not only with the disorder, but also remission. As such, it is proposed an improved definition would not only better reflect the content of the DSM-5, but could also contribute in a better understanding of assessment and practice of diagnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The dataset consists in a small corpus composed with the most relevant chapters of DSM-5. Computational Text mining method and manual qualitative approach were used in a processing chain for the conceptual analysis of SYMPTOM. Firstly, extraction of the textual data from noisy sources and cleaning of the text were performed. Secondly, all sentences with the term "symptom" in it, plus one sentence before and after, were extracted, yielding a set of textual segments, on which stemming and lemmatisation were performed. Thirdly, the pretreated data was used to create a document-term matrix with the TF-IDF weighting scheme. Fourthly, from the matrix, textual segment clusters were produced with the k-means algorithm. Fifthly, the most salient words in each cluster and in the whole subcorpus were first represented using word clouds. Finally, relations of similarity between the most relevant words in each cluster and in the subcorpus were represented in a 3d space. All steps were performed using common R modules (tm, RWeka, qdap, cluster, knn, ggplot2, rgl). Each cluster was interpreted as a specific field in which a hypothetical conceptual property of SYMPTOM is expressed. Categorization was done by annotating manually the most typical textual segments in every cluster according to cosine distance to the centroid. Annotations consisted in the main conceptual property of SYMPTOM expressed in a segment.</p><p>Syntheses of these annotations were done for each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimentation</head><p>From the subcorpus, 2036 sentences containing the term "symptom" were extracted which contained 5761 different word types. The words most associated with symptom in this subset of the corpus are disord (disorder), criteria, presen (presence), sever (severity), medic (medical, medication). Using k-means, 30 clusters were extracted but 17 are deemed noisy, as they contain 10 textual segments or less. Most of the remaining clusters are located in the section II of the DSM (diagnostic criteria and codes), several having most of their segments in a specific disorder chapter. The converse, however, does not hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Analysing those clusters reveals that SYMPTOM has a transdiagnostic property, and is not only defined by its specific content. For example, let us examine cluster #8, which contains 98 segments, 90% of which fall in the three chapters about psychotic and mood disorders. Symptom in this cluster is linked with the words depress, hypomania, mania, but also with episode, period, full, meet. Furthermore, annotation of joined documents shows that temporality is a conceptual property of SYMPTOM. It modifies the pathological dimension of its content. SYMPTOM is not in direct causal relation with DISORDER; it is a dynamic sign whose presence needs to be situated in an episode, regardless of whether its content is depressive or manic. Therefore, the mere presence of a symptom is not sufficient for a diagnosis. Conversely, SYMPTOM is also linked with the concept of (partial) remission. SYMPTOM and DIS-ORDER have a complex relationship on a continuum between negative (remission) and positive (disease) poles.</p><p>In conclusion, a mixed method, combining computational and manual processing and using quantitative and qualitative approaches, was applied in our conceptual analysis of the concept SYMPTOM. SYMPTOM appears to be a more complex and dynamic concept than patients and other non-doctors usually understand it to be. As a result, a better understanding of this complexity would likely profit assessment, diagnosis and treatment of mental disorders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnostic and Statistical Manual of Mental Disorders</title>
	</analytic>
	<monogr>
		<title level="j">American Psychiatric Association</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>American Psychiatric Association</publisher>
		</imprint>
	</monogr>
	<note>5th ed. Washington, DC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">La formation de l&apos;esprit scientifique. Contribution à une psychanalyse de la connaissance objective. Vrin, Paris; The Formation of the Scientific Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bachelard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinamen</title>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic Modeling and Digital Humanities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Humanities</title>
		<imprint>
			<date type="published" when="2013-04-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Narrative Event Chains. ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<title level="m">Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors. ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Not-so-Latent Dirichlet Allocation: Collapsed Gibbs Sampling Using Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
	<note>CSLDAMT &apos;10. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading Tea Leaves: How Humans Interpret Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text Mining Methods for Social Representation Analysis in Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Chartier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Papers on Social Representations</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="38" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CARCAT : Computer-Assisted Reading and Conceptual Analysis of Texts : An experiment applied to the concept of evolution in the work of Henri Bergson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Studies/Le champ numériquenumérique</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">L&apos;archeólogie du savoir. Gallimard, Paris; The Archaeology of Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Foucault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<pubPlace>Routledge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ModelBased Reasoning in Scientific Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giere</surname></persName>
		</author>
		<editor>L. Magnani, N. Nersessian and P. Thagard,</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Plenum Publishers</publisher>
			<biblScope unit="page" from="41" to="57" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Using Models to Represent Reality</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling Biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">What is in a Model? Combining Theoretical and Material Models to Develop Intelligible, Modeling Biology: Structures, Behaviors, Evolution</title>
		<editor>Manfred Dietrich Laubichler, Gerd B. Mü lle</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Humanities Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mccarthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Palgrave MacMillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Models as mediators. Perspectives on natural and social science</title>
		<editor>Morgan, M.S., and Morrison, M.</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arts et sciences du texte</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">;</forename><surname>Rastier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Puf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rastier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Saussure au futur. Les belles lettres</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rockwell</surname></persName>
		</author>
		<title level="m">What is Text Analysis, Really? Literary and Linguistic Computing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Analyse de donneés textuelles sous R. ISTE Ew ditions, coll. Sciences Cognitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turenne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Londres</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conceptualisation and Evolution of Concepts. The example of French Linguist Gustave Guillaume, Academic discourse -multidisciplinary approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valette</surname></persName>
		</author>
		<editor>Kj. Fløttum &amp; F. Rastier</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Novus Press</publisher>
			<biblScope unit="page" from="55" to="74" />
			<pubPlace>Oslo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Propositions pour une approche textuelle de la conceptualisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Actes des 21es Journeés francophones d&apos;IngénierieIngénierie des Connaissances (IC&apos;2010) (8-11 juin 2010)</title>
		<meeting>s des 21es Journeés francophones d&apos;IngénierieIngénierie des Connaissances (IC&apos;2010) (8-11 juin 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5" to="16" />
		</imprint>
	</monogr>
	<note>Nı̂ mes Sylvie Despres, édéd. Publication de l&apos;Ecole des Mines d&apos; Alè s</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Geometry and Meaning. CSLI Publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Center for the Study of Language and Information, Leland Stanford Junior University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
