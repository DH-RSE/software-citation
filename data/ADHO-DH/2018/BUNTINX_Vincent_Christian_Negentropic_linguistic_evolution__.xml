<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Negentropic linguistic evolution: A comparison of seven languages</title>
                <author>
                    <persName>
                        <surname>Buntinx</surname>
                        <forename>Vincent</forename>
                    </persName>
                    <affiliation>EPFL (École polytechnique fédérale de Lausanne), Switzerland</affiliation>
                    <email>vincent.buntinx@epfl.ch</email>
                </author>
                <author>
                    <persName>
                        <surname>Kaplan</surname>
                        <forename>Frédéric</forename>
                    </persName>
                    <affiliation>EPFL (École polytechnique fédérale de Lausanne), Switzerland</affiliation>
                    <email>frederic.kaplan@epfl.ch</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2018-04-25T08:00:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Name, Institution</publisher>
                <address>
                    <addrLine>Street</addrLine>
                    <addrLine>City</addrLine>
                    <addrLine>Country</addrLine>
                    <addrLine>Name</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>kernel entropy</term>
                    <term>linguistic evolution</term>
                    <term>Google Books</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>corpora and corpus activities</term>
                    <term>multilingual / multicultural approaches</term>
                    <term>text analysis</term>
                    <term>linguistics</term>
                    <term>English</term>
                    <term>computer science</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p rend="Plain Text">Introduction</p>
            <p rend="Plain Text">The relationship between the entropy of language and its complexity has been the subject of much speculation – some seeing the increase of linguistic entropy as a sign of linguistic complexification or interpreting entropy drop as a marker of greater regularity (Montemurro and Zanette 2011, Juola 2016, Bentz et al. 2017). Some evolutionary explanations, like the learning bottleneck hypothesis, argues that communication systems having more regular structures tend to have evolutionary advantages over more complex structures (Kirby 2001, Tamariz and Kirby 2016, Ferrer I Cancho 2017). Other structural effects of communication networks, like globalization of exchanges or algorithmic mediation, have been hypothesized to have a regularization effect on language (Kaplan 2014). </p>
            <p rend="Plain Text">Longer-term studies are now possible thanks to the arrival of large-scale diachronic corpora, like newspaper archives or digitized libraries (Westin and Geisler 2002, Fries and Lehmann 2006, Lyse and Andersen 2012, Rochat et al. 2016). However, simple analyses of such datasets are prone to misinterpretations due to significant variations of corpus size over the years and the indirect effect this can have on various measures of language change and linguistic complexity (Buntinx et al. 2017). In particular, it is important not to misinterpret the arrival of new words as an increase in complexity as this variation is intrinsical, as is the variation of corpus size.</p>
            <p rend="Plain Text">This paper is an attempt to conduct an unbiased diachronic study of linguistic complexity over seven different languages using the Google Books corpus (Michel et al. 2011). The paper uses a simple entropy measure on a closed, but nevertheless large, subset of words, called kernels (Buntinx et al. 2016). The kernel contains only the words that are present without interruption for the whole length of the study. This excludes all the words that arrived or disappeared during the period. We argue that this method is robust towards variations of corpus size and permits to study change in complexity despite possible (and in the case of Google Books unknown) change in the composition of the corpus. Indeed, the evolution observed on the seven different languages shows rather different patterns that are not directly correlated with the evolution of the size of the respective corpora. The rest of the paper presents the methods followed, the results obtained and the next steps we envision.</p>
            <p rend="Plain Text">Method and Results</p>
            <p rend="Plain Text">We use the concept of kernel entropy (Buntinx et al. 2017), defined as the Shannon entropy measure applied on word occurrences distribution normalized on the kernel of a given corpus. To calculate this measure, the corpus is subdivided into yearly sub-corpora. Next, we then calculate the word occurrences for the words that are present in each sub-corpus for each year. These words form a set, called a kernel. The word frequencies are normalized on the kernel 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">K</mi>
                    </mml:math>
                </formula> for each year 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">y</mi>
                    </mml:math>
                </formula> and the formula of Shannon entropy (using napierian logarithm) is applied on these distributions providing a measure that can be compared diachronically with robustness to corpus size evolution and to noises. The kernel entropy of a kernel 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">K</mi>
                    </mml:math>
                </formula> for the year 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">y</mi>
                    </mml:math>
                </formula> is given by the formula:
            </p>
            <figure>
                <graphic n="1001" width="5.2535694444444445cm" height="1.7991666666666666cm" url="Pictures/9382f9d785310041029d0a0f99312472.png" rend="inline"/>
            </figure>
            <p rend="No Spacing">Where 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <msup xmlns="http://www.w3.org/1998/Math/MathML">
                            <mrow>
                                <mi>N</mi>
                            </mrow>
                            <mrow>
                                <mi>K</mi>
                            </mrow>
                        </msup>
                    </mml:math>
                </formula> is the number of words composing the kernel and 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <msubsup xmlns="http://www.w3.org/1998/Math/MathML">
                            <mrow>
                                <mi>f</mi>
                            </mrow>
                            <mrow>
                                <mi>i</mi>
                            </mrow>
                            <mrow>
                                <mi>K</mi>
                                <mo>,</mo>
                                <mi>y</mi>
                            </mrow>
                        </msubsup>
                    </mml:math>
                </formula> the relative occurrence frequency of the word 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">i</mi>
                    </mml:math>
                </formula> normalized on the kernel 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">K</mi>
                    </mml:math>
                </formula> in the year 
                <formula>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML">
                        <mi xmlns="http://www.w3.org/1998/Math/MathML">y</mi>
                    </mml:math>
                </formula>. The kernel entropy measure is computed for seven languages of Google Books corpora. 
                <hi rend="italic">Figure 1</hi> shows the kernel entropy variations normalized with respect to the average value (which change over the languages because kernels of different corpus also have different sizes).
            </p>
            <figure>
                <graphic n="1002" width="16.59113888888889cm" height="10.851319444444444cm" url="Pictures/c82342d8bfd9d6f4d2aa53ca9b3e2c44.png" rend="inline"/>
            </figure>
            <p rend="No Spacing">Figure 1: Normalized yearly kernel entropy evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
            <p rend="No Spacing">We observe that even if all the seven language have different patterns and inflection points, they tend generally to show an effect of negentropy with increasing years. We note that most languages have a crosspoint in 1905, except for the Russian language, showing variations particularly from 1920 to 1930. We present in Figure 2 the kernel entropy evolution for each language in comparison to the corpus size.</p>
            <table rend="rules">
                <row>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1003" width="8cm" height="5.23cm" url="Pictures/7b52952f7cacae749a03fa7476bc5823.png" rend="inline"/>
                        </figure>
                    </cell>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1004" width="8cm" height="5.23cm" url="Pictures/5d5242dd47387cc86d72fcbd120df78d.png" rend="inline"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1005" width="8cm" height="5.23cm" url="Pictures/44c2bb28b891812dd67889713eb09444.png" rend="inline"/>
                        </figure>
                    </cell>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1006" width="8cm" height="5.23cm" url="Pictures/a9231b34e46635f6c849b195b13ea640.png" rend="inline"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1007" width="8cm" height="5.23cm" url="Pictures/269c781015d6cc5c0781a55e16246a78.png" rend="inline"/>
                        </figure>
                    </cell>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1008" width="8cm" height="5.23cm" url="Pictures/8dbd3e0ba863083cc8823e3e2183b5ab.png" rend="inline"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="No_Spacing">
                        <p rend="No Spacing"> Legend:</p>
                        <p rend="No Spacing">
                            <hi rend="bold">(1) British / American English</hi>
                        </p>
                        <p rend="No Spacing">
                            <hi rend="bold">(2) French / Italian</hi>
                        </p>
                        <p rend="No Spacing">
                            <hi rend="bold">(3) Spanish / German</hi>
                        </p>
                        <p rend="No Spacing">
                            <hi rend="bold">(4) Russian</hi>
                        </p>
                        <p rend="No Spacing">
                            <hi rend="bold color(0070C0)">Kernel Entropy: Blue</hi>
                        </p>
                        <p rend="No Spacing">
                            <hi rend="bold color(FF0000)">Size: Red</hi>
                        </p>
                    </cell>
                    <cell rend="No_Spacing">
                        <figure>
                            <graphic n="1009" width="8cm" height="5.23cm" url="Pictures/d225ad506ffb4dc7f94c08c35db5a165.png" rend="inline"/>
                        </figure>
                    </cell>
                </row>
            </table>
            <p rend="No Spacing">Figure 2: Yearly kernel entropy evolution and size evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
            <p>Google Books corpora may experience sudden changes in composition depending on the year. For example, the addition of scientific literature and medical journals (Pechenick et al., 2015). In this case, the words kernel distribution, even if it is robust because composed of the most stable words, can change for a year which is subject to a change of composition of the corpus. However, this effect is still reduced because the words appearing and disappearing during this transition phase are not taken into account. We observe that the entropy of the kernel seems not to be affected by the size variations of corpora and when it appear to be affected, the direction of variation is unpredictable.</p>
            <p>The British English and American English are the least affected languages by the negentropic effect. Their kernel entropy increases over time until 1960 (British English) and 1940 (American English). However, American English kernel entropy decrease quickly from 1940 to 1985. We observe that the obtained curve for the French language is similar to the one corresponding to the study of language evolution through 200 years of newspapers written in French despite a different kernel size (Buntinx et al. 2017). </p>
            <p>Interesting inflection points are detected and should be poignant to specialists of the targeted language. We present in 
                <hi rend="italic">Figure 3</hi> the number of words in the kernel and inflections points for the seven languages.
            </p>
            <table rend="rules">
                <row>
                    <cell>Language</cell>
                    <cell>Number of words in the kernel</cell>
                    <cell>Inflection point 1</cell>
                    <cell>Inflection point 2</cell>
                </row>
                <row>
                    <cell>British English</cell>
                    <cell>82’332</cell>
                    <cell>1959</cell>
                    <cell/>
                </row>
                <row>
                    <cell>American English</cell>
                    <cell>44’949</cell>
                    <cell>1931</cell>
                    <cell>1985</cell>
                </row>
                <row>
                    <cell>French</cell>
                    <cell>79’575</cell>
                    <cell>1825</cell>
                    <cell>1885</cell>
                </row>
                <row>
                    <cell>German</cell>
                    <cell>36’660</cell>
                    <cell>1850</cell>
                    <cell>1946</cell>
                </row>
                <row>
                    <cell>Italian</cell>
                    <cell>30’996</cell>
                    <cell>1983</cell>
                    <cell/>
                </row>
                <row>
                    <cell>Spanish</cell>
                    <cell>25’582</cell>
                    <cell>1995</cell>
                    <cell/>
                </row>
                <row>
                    <cell>Russian</cell>
                    <cell>5’123</cell>
                    <cell>1920</cell>
                    <cell>1988</cell>
                </row>
            </table>
            <p rend="No Spacing">Figure 3: Number of words in the kernel and kernel entropy inflection points for the seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.</p>
            <p>Furthermore, it is possible to show the languages proximity in terms of kernel entropy evolution behavior through the determination of a distance based on kernel entropy correlations. A projection of the resulting matrix distance using PCA is presented in Figure 4.</p>
            <p rend="Plain Text">We observe that British English and American English are represented together to the left of the plan because they have a relative opposite pattern with respect to other languages. Russian is also particular because of the brutal effect of the negentropy observed between around 1920 and the sudden increase at the end of the 1980s. The last four languages, French, Spanish, German and Italian share a more similar behavior and are represented in the right-bottom part of the plan. </p>
            <p rend="Plain Text">Although much more in-depth investigation must be done, it is reasonable to make the hypothesis of different internal and external factors for explaining these various patterns. The Russian case clearly invites to investigate correlations between linguistic policies during the Sovietic period and their actual effects of the Russian language.</p>
            <p rend="Plain Text">The similarity between French, German, Italian and Spanish pushes in the direction for similar processes of standardization, potentially due to linguistic convergence at national levels suppressing some regional particularities. In contrast, American and British English evolution is likely to be explained through the particular histories of the respective English-speaking populations and their relation to the rest of world. The progressive rise of English as a global language, spoken and written by many non-native speakers, is certainly playing a role in the shaping these particular curves. </p>
            <figure>
                <graphic n="10010" width="14.515041666666667cm" height="10.489847222222222cm" url="Pictures/20159d811a1630a94bf5b3e732e5273d.png" rend="inline"/>
            </figure>
            <p rend="No Spacing">Figure 4: PCA projection of distance matrix using kernel entropy correlation-based distance for Google Books corpora: British English, US English, French, German, Italian, Spanish and Russian.</p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl rend="No Spacing">C. Bentz, D. Alikaniotis, M. Cysouw and R. Ferrer-i-Cancho. The Entropy of Words—Learnability and Expressivity across More Than 1000 Languages. Entropy, 19(6), 275, 2017.</bibl>
                    <bibl rend="No Spacing">V. Buntinx, C. Bornet and F. Kaplan. Studying Linguistic Changes on 200 Years of Newspapers. 
                        <hi rend="italic">DH2016</hi>, Kraków, Poland, July 11-16, 2016. 
                    </bibl>
                    <bibl rend="No Spacing">V. Buntinx
                        <hi rend="infoscience_authors" xml:space="preserve">, </hi>F. Kaplan 
                        <hi rend="infoscience_authors" xml:space="preserve">and </hi>A. Xanthos 
                        <hi rend="infoscience_authors">(Dirs.).</hi> Analyse multi-échelle de n-grammes sur 200 années d'archives de presse. Thèse EPFL, n° 8180, 2017.
                    </bibl>
                    <bibl rend="No Spacing">R. Ferrer-i-Cancho. Optimization models of natural communication. Journal of Quantitative Linguistics, 1-31, 2017.</bibl>
                    <bibl rend="No Spacing">U. Fries and H. M. Lehmann. The style of 18th century english newspapers: Lexical diversity. News Discourse in Early Modern Britain, pages 91–104, 2006.</bibl>
                    <bibl rend="No Spacing">P. Juola. Using the Google N-Gram corpus to measure cultural complexity. Literary and linguistic computing, 28(4), 668-675, 2013.</bibl>
                    <bibl rend="No Spacing">F. Kaplan. Linguistic capitalism and algorithmic mediation. Representations, 127 (1):57–63, 2014.</bibl>
                    <bibl rend="No Spacing">S. Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, vol. 5, no 2, p. 102-110, 2001.</bibl>
                    <bibl rend="No Spacing">G. I. Lyse and G. Andersen. Collocations and statistical analysis of n-grams. Exploring Newspaper Language: Using the Web to Create and Investigate a Large Corpus of Modern Norwegian. Studies in Corpus Linguistics, John Benjamins Publishing, Amsterdam, pages 79–109, 2012.</bibl>
                    <bibl rend="No Spacing">J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, 
                        <name>D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. A. Nowak</name> and E. Lieberman-Aiden. Quantitative analysis of culture using millions of digitized books. 
                        <hi rend="italic">science</hi>, 
                        <hi rend="italic">331</hi>(6014), 176-182, 2011.
                    </bibl>
                    <bibl rend="No Spacing">M. A. Montemurro and D. H. Zanette. Universal entropy of word ordering across linguistic families. PLoS One, 6(5), e19875, 2011.</bibl>
                    <bibl rend="No Spacing">E. A. Pechenick, C. M. Danforth and P. S. Dodds. Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution. PloS one, 10(10), e0137041, 2015.</bibl>
                    <bibl rend="No Spacing">Y. Rochat, M. Ehrmann, V. Buntinx, C. Bornet and F. Kaplan. Navigating through 200 years of historical newspapers. In iPRES 2016, numéro EPFL-CONF-218707, 2016.</bibl>
                    <bibl rend="No Spacing">M. Tamariz and S. Kirby. The cultural evolution of language. Current Opinion in Psychology 8: 37-43, 2016.</bibl>
                    <bibl rend="No Spacing">I. Westin and C. Geisler. A multi-dimensional study of diachronic variation in british newspaper editorials. International Computer Archive of Modern and Medieval English, (26):133–152, 2002.</bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
