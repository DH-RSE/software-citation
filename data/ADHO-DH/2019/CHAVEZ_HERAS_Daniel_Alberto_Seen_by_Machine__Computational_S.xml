<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Seen by Machine: Computational Spectatorship in the BBC television archive</title>
                <author>
                    <persName>
                        <surname>Chavez Heras</surname>
                        <forename>Daniel Alberto</forename>
                    </persName>
                    <affiliation>King's College London, United Kingdom</affiliation>
                    <email>daniel.chavez@kcl.ac.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Blanke</surname>
                        <forename>Tobias</forename>
                    </persName>
                    <affiliation>King's College London, United Kingdom</affiliation>
                    <email>tobias.blanke@kcl.ac.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Cowlishaw</surname>
                        <forename>Tim</forename>
                    </persName>
                    <affiliation>BBC, United Kingdom</affiliation>
                    <email>Tim.Cowlishaw@bbc.co.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Fiala</surname>
                        <forename>Jakub</forename>
                    </persName>
                    <affiliation>BBC, United Kingdom</affiliation>
                    <email>jakub.fiala@bbc.co.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Herranz Donnan</surname>
                        <forename>Amaya</forename>
                    </persName>
                    <affiliation>BBC, United Kingdom</affiliation>
                    <email>amaya.herranzdonnan@bbc.co.uk</email>
                </author>
                <author>
                    <persName>
                        <surname>Man</surname>
                        <forename>David</forename>
                    </persName>
                    <affiliation>BBC, United Kingdom</affiliation>
                    <email>david.man@bbc.co.uk</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2019-04-29T17:44:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Name, Institution</publisher>
                <address>
                    <addrLine>Street</addrLine>
                    <addrLine>City</addrLine>
                    <addrLine>Country</addrLine>
                    <addrLine>Name</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>computational spectatorship</term>
                    <term>machine learning</term>
                    <term>television</term>
                    <term>computationla creativity</term>
                    <term>cultural analytics</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>audio</term>
                    <term>video</term>
                    <term>multimedia</term>
                    <term>film and performing arts studies</term>
                    <term>cultural artifacts digitisation - theory</term>
                    <term>methods and technologies</term>
                    <term>English</term>
                    <term>computer science and informatics</term>
                    <term>artificial intelligence and machine learning</term>
                    <term>cultural analytics</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <div type="div1" rend="DH-Heading1">
                <head>Summary</head>
                <p style="text-align:left; ">This paper is a report and critical account of research undertaken in the project 
                    <hi rend="italic">Made by Machine: When AI Met the Archive</hi> (
                    <ref target="https://www.bbc.co.uk/programmes/b0bhwk3p">https://www.bbc.co.uk/programmes/b0bhwk3p</ref>). In this project we used three computational approaches to analyse and automatically browse the BBC television archive; we then proposed a novel way of combining these approaches through machine learning by fitting their outputs to a recurrent neural network as a time-series.
                </p>
                <p style="text-align:left; ">Through these methods, we were able to generate new sequences out of a dataset of short clips of archive footage. These sequences were then edited and packaged into a television programme broadcast on BBC Four in September 2018. This is, to our knowledge, the first time machine learning has been used in this way to recast archive footage into original prime-time content for television.</p>
                <p style="text-align:left; ">In this paper, we first frame 
                    <hi rend="italic">Made by Machine</hi> as a project inscribed in the political economy of datafied cultural production. We then describe the technological approaches we used to traverse archive space, learn and extract features from video, and model their relations through time. And finally, we introduce the idea of 
                    <hi rend="bold">computational spectatorship</hi> as a concept to analyse the objects and practices of automated seeing/editing of moving imagery through machine learning.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>The television archive as cultural big data</head>
                <p style="text-align:left; ">In public discourse we are constantly reminded that how we learn, what we enjoy, and the ways we conceive and exercise power, are all consistently mediated by imagery. In various academic disciplines this pre-eminence of imagery in all spheres of human activity has been referred to as visual culture or the visual turn (Mirzoeff, 2002; Jay, 2002). It has been further recognised that digital and networked technologies have dramatically increased the role imagery plays in society. According to an estimate already 23.2% of files available online in 2003 were images (Lyman and Varian, 2003) and this is before the rise of YouTube and Netflix in the mid-2000s. A more recent report by CISCO predicts that by 2022 video will account for more than 80% of the world’s IP traffic. If this is the case, by then we will be collectively watching through Video-on-Demand (VoD) systems the equivalent to 10 billion DVDs per month (CISCO, 2018: 2).</p>
                <p style="text-align:left; ">Arguably, out of all the cognitive-cultural production today, moving imagery is the most semantically ubiquitous, the larger in size, and one of the most semiotically complex insofar as it combines visual, linguistic, and aural modes of expression. In their abundance and complexity moving images are in today’s networked societies one of the most challenging aspects of contemporary culture, and one that increasingly demands social and academic attention. In this context, while the volume of digital video grows, and its formats continue to diversify, the use of computational techniques to access, analyse, produce and reproduce vast collections of digital moving imagery becomes a pressing issue for organisations and archives that deal with audio-visual production (see: Kuhn et al., 2015; and Ward and Barker, 2013)</p>
                <p style="text-align:left; ">One of such collections of is the BBC television archive, which amounts to approximately 700,000 hours of television distributed in about 400,000 programmes in various formats, including items originally stored in film and magnetic tapes that have now been digitised, and new programming which is preserved as born-digital data (British Film Institute, 2018; Lee, 2014). A significant portion of this archive is now available through the internal 
                    <hi rend="italic">Redux</hi> system, which allows on-demand access to high-resolution video stream files through an online interface (Butterworth, 2008).
                </p>
                <p style="text-align:left; ">In the rapidly changing landscape of digital entertainment industries, where large corporations like 
                    <hi rend="italic">Netflix</hi> or 
                    <hi rend="italic">Amazon</hi> are producing original video content and are —in industry parlance— leveraging their own moving image collections as profitable datasets, the BBC is increasingly pushed to recognise the value of their archives as data. In 2017, BBC R&amp;D signed a five-year partnership with eight UK universities to “unlock the potential of data in the media” by creating a framework to make BBC data available for research (Chadwick in BBC R&amp;D, 2017). To unlock such potential can be understood in this context as extracting value from this data, but unlike 
                    <hi rend="italic">Amazon</hi> or 
                    <hi rend="italic">Netflix</hi>, the BBC is a public corporation funded largely by tax-payers to fulfil public purposes (
                    <ref target="https://www.bbc.com/aboutthebbc/governance/charter">https://www.bbc.com/aboutthebbc/governance/charter</ref>). The British broadcaster therefore needs to grapple with the questions of what types of data and what kinds of value it extracts from its television archives. These questions require the broader intellectual frameworks of the humanities to place 
                    <hi rend="italic">Made by Machine</hi> into context.
                </p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Seen by Machine</head>
                <p style="text-align:left; ">Made by Machine was a project commissioned under a simple premise: to create a TV programme out of archive footage using machine learning. The project was conceived, produced and delivered between May and August 2018, and it aired under the category of documentary on BBC Four in September as part of a two-night experimental programming block called “AITV on BBC 4.1” (
                    <ref target="https://www.bbc.co.uk/programmes/p06jt9ng">https://www.bbc.co.uk/programmes/p06jt9ng</ref>).
                </p>
                <p style="text-align:left; ">Most of 
                    <hi rend="italic">Made by Machine</hi> was developed internally by a small group of technologists at BBC R&amp;D, in collaboration with an external researcher from King’s College London. This team designed the computational methods to analyse archive footage and produce new video sequences out of it; the sequences were then packaged and delivered as a documentary by a producer who commissioned a 3D-rendered talking head and a presenter whose commentary bookends the sequences.
                </p>
                <p style="text-align:left; ">First, a Support Vector Machine (SVM) was trained on existing programme metadata “to predict whether or not a given programme was broadcast on BBC Four” (Cowlishaw, 2018). Using this, an ad hoc subset of programmes was selected, and then segmented into several thousand clips that ranged from tens of seconds to several minutes using an algorithm developed internally at BBC R&amp;D (Cowlishaw, 2018). These clips were then used as a dataset to be explored and manipulated using four methods:</p>
                <p style="text-align:left; ">1. Object detection</p>
                <p style="text-align:left; ">2. Subtitle analysis</p>
                <p style="text-align:left; ">3. Visual energy</p>
                <p style="text-align:left; ">4. Mixed (generative) model</p>
                <p rend="DH-Heading3" style="font-weight: bold; font-style: italic; ">Object detection</p>
                <p style="text-align:left; ">The 
                    <hi rend="italic">Densecap</hi> system (
                    <ref target="https://cs.stanford.edu/people/karpathy/densecap/">https://cs.stanford.edu/people/karpathy/densecap/</ref>) was used to automatically annotate the dataset of clips and then these annotations were used as metadata to generate the sequences by similarity. 
                    <hi rend="italic">Densecap</hi> is a computer vision system designed to “localize and describe salient regions in images in natural language” (Johnson et al., 2015, p.1). It is trained on the visual genome dataset (
                    <ref target="https://visualgenome.org/">https://visualgenome.org/</ref>), a crowd-sourced set of densely annotated images.
                </p>
                <p rend="DH-Heading3" style="font-weight: bold; font-style: italic; ">Subtitle analysis</p>
                <p style="text-align:left; ">A pre-trained 
                    <hi rend="italic">Word2Vec</hi> language model (
                    <ref target="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</ref>) was used to create word embeddings of the subtitles of the programmes, and then term frequency–inverse document frequency (TFIDF) to retrieve clips and generate the sequences based on word salience and similarity.
                </p>
                <p rend="DH-Heading3" style="font-weight: bold; font-style: italic; ">Visual energy</p>
                <p style="text-align:left; ">MPEG-2 video encoding was used to extract a frame-to-frame motion vector of pixel colour difference. This signal was then used to rank every clip in the dataset according to this representation of video dynamics (also preserving the variation information at the frame level).</p>
                <p rend="DH-Heading3" style="font-weight: bold; font-style: italic; ">Mixed mode</p>
                <p style="text-align:left; ">The features learned/extracted from the previous three sections were then used as inputs to train a long short-term memory (LSTM) machine (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) to learn a model of the relations between the features across time. The model was later used to generate new similar sequences.</p>
                <p style="text-align:left; ">Through these four techniques the team aimed to represent digital video in terms of three of its dimensions: depiction (visual), dialogue (linguistic) and motion (plastic). One significant contribution in our approach is to try to model moving imagery as a multi-dimensional time-series problem: analysing video streams as a sequence of para-symbolic units whose ordering in time is integral to the creation of meaning.</p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Computational spectatorship</head>
                <p style="text-align:left; ">Finally, we conceptualise this family of machine-learning approaches as machine-seers. These machine-seers mediate our relationship with large audio-visual collections and provide novel ways of automatic browsing as a form of automated editing.</p>
                <p style="text-align:left; ">We argue that machine-seers enable a new and specific modality of vision, one which depends heavily on machine-learned representations not only of digital video features but, crucially, of spectatorship. Machine-seers are built on top of existing computer vision and other machine learning techniques, but we conceive them as a second generation of systems designed to learn and replicate complex viewing practices in and across media; they activate in audiences new visual dispositions and capacities, and as such they enable particular ways of seeing (see: Berger, 1972). </p>
                <p style="text-align:left; ">One significant advantage of framing a project like 
                    <hi rend="italic">Made by Machine</hi> in this way is that theories of spectatorship enable us to think of the imagery produced through machine learning systems as images of the systems themselves; their aesthetics include the technological and social conditions for their existence.
                </p>
                <p style="text-align:left; ">We therefore argue that 
                    <hi rend="italic">Made by Machine</hi> can be understood as one early instance of a new aesthetic modality: the datamatic (Ikeda et al., 2012). Unlike the cinematic or the televisual before it, the datamatic is first and foremost a form of computational spectatorship: more than novel ways of 
                    <hi rend="bold">creating</hi> imagery, machine-seers afford us with novel ways of 
                    <hi rend="bold">enjoying</hi> imagery; they fetishise calculation and turn the datafication of society into its own form of spectacle. Through this 
                    <hi rend="italic">spectaculum ex computatio</hi>, datamatic watching allows us, potentially, to enjoy sequencing without continuity, narrative without authorship and, ultimately, presence without subject.
                </p>
            </div>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style="text-align:left; ">BBC R&amp;D (2017) Data Science Research Partnership [online]. Available from: 
                        <ref target="https://www.bbc.co.uk/rd/projects/data-science-research-partnership">https://www.bbc.co.uk/rd/projects/data-science-research-partnership</ref> (Accessed 28 September 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Berger, J. (1972) Ways of Seeing. 2008 edition. Penguin UK.</bibl>
                    <bibl style="text-align:left; ">British Film Institute (2018) Key archives and what they do [online]. Available from: 
                        <ref target="https://www.bfi.org.uk/archive-collections/archive-projects/artist-archive/key-archives-what-they-do">https://www.bfi.org.uk/archive-collections/archive-projects/artist-archive/key-archives-what-they-do</ref> (Accessed 15 October 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Butterworth, B. (2008) History of the ‘BBC Redux’ project [online]. Available from: 
                        <ref target="http://www.bbc.co.uk/blogs/bbcinternet/2008/10/history_of_the_bbc_redux_proje.html">http://www.bbc.co.uk/blogs/bbcinternet/2008/10/history_of_the_bbc_redux_proje.html</ref> (Accessed 15 October 2018).
                    </bibl>
                    <bibl style="text-align:left; ">CISCO (2018) Cisco Visual Networking Index: Forecast and Trends. [online]. Available from: 
                        <ref target="https://www.cisco.com/c/m/en_us/solutions/service-provider/vni-forecast-highlights.html%20">https://www.cisco.com/c/m/en_us/solutions/service-provider/vni-forecast-highlights.html</ref> (Accessed 22 January 2019).
                    </bibl>
                    <bibl style="text-align:left; ">Cowlishaw, T. (2018) Using Artificial Intelligence to Search the Archive. BBC R&amp;D Blog [online]. Available from: 
                        <ref target="https://www.bbc.co.uk/rd/blog/2018-10-artificial-intelligence-archive-television-bbc4">https://www.bbc.co.uk/rd/blog/2018-10-artificial-intelligence-archive-television-bbc4</ref> (Accessed 27 September 2018). 
                    </bibl>
                    <bibl style="text-align:left; ">Gers, F. A. et al. (2000) Learning to Forget: Continual Prediction with LSTM. Neural Computation. [Online] 12 (10), 2451–2471.</bibl>
                    <bibl style="text-align:left; ">Hochreiter, S. &amp; Schmidhuber, J. (1997) Long Short-Term Memory. Neural Computation. [Online] 9 (8), 1735–1780.</bibl>
                    <bibl style="text-align:left; ">Ikeda, R. et al. (2012) Ryoji Ikeda: Datamatics. Charta.</bibl>
                    <bibl style="text-align:left; ">Jay, M. (2002) Cultural relativism and the visual turn. Journal of Visual Culture. [Online] 1 (3), 267–278.</bibl>
                    <bibl style="text-align:left; ">Johnson, J. et al. (2015) DenseCap: Fully Convolutional Localization Networks for Dense Captioning. arXiv:1511.07571 [cs]. [online]. Available from: 
                        <ref target="http://arxiv.org/abs/1511.07571%20">http://arxiv.org/abs/1511.07571</ref> (Accessed 23 November 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Kuhn, V. et al. (2015) ‘The VAT: Enhanced Video Analysis’, in Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. XSEDE ’15. pp. 11:1–11:4. [online]. Available from: 
                        <ref target="http://doi.acm.org/10.1145/2792745.2792756">http://doi.acm.org/10.1145/2792745.2792756</ref> (Accessed 23 November 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Lee, A. (2014) An interview with Adam Lee, BBC archive expert [online]. Available from: 
                        <ref target="http://www.bbc.co.uk/archive/tv_archive.shtml%20">http://www.bbc.co.uk/archive/tv_archive.shtml</ref> (Accessed 15 October 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Lyman, P. &amp; Varian, H. R. (2003) How Much Information. [online]. Available from: 
                        <ref target="http://groups.ischool.berkeley.edu/archive/how-much-info-2003/%20">http://groups.ischool.berkeley.edu/archive/how-much-info-2003/</ref> (Accessed 23 November 2018).
                    </bibl>
                    <bibl style="text-align:left; ">Mirzoeff, N. (2002) The Visual Culture Reader. Psychology Press.</bibl>
                    <bibl style="text-align:left; ">Ward, J. S. &amp; Barker, A. (2013) Undefined By Data: A Survey of Big Data Definitions. arXiv:1309.5821 [cs]. [online]. Available from: 
                        <ref target="http://arxiv.org/abs/1309.5821">http://arxiv.org/abs/1309.5821</ref> (Accessed 17 October 2018).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
