<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">I-Media-Cities: A Digital Ecosystem Enriching A Searchable Treasure Trove Of Audio Visual Assets</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Scipione</surname>
                        <forename>Gabriella</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>g.scipione@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Guidazzoli</surname>
                        <forename>Antonella</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>a.guidazzoli@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Imboden</surname>
                        <forename>Silvano</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>s.imboden@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Trotta</surname>
                        <forename>Giuseppe</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>g.trotta@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Montanari</surname>
                        <forename>Margherita</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>m.montanari@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Liguori</surname>
                        <forename>Maria Chiara</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>m.liguori@cineca.it</email>
                </author>
                <author>
                    <persName>
                        <surname>Caraceni</surname>
                        <forename>Simona</forename>
                    </persName>
                    <affiliation>Cineca, Italy</affiliation>
                    <email>s.caraceni@cineca.it</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2019-04-01T13:17:08.401244241</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Name, Institution</publisher>
                <address>
                    <addrLine>Street</addrLine>
                    <addrLine>City</addrLine>
                    <addrLine>Country</addrLine>
                    <addrLine>Name</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Multimedia and multimodal retrieval</term>
                    <term>Automatic tagging</term>
                    <term>Search interfaces</term>
                    <term>User interface management systems</term>
                    <term>Media arts</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>film and performing arts studies</term>
                    <term>metadata</term>
                    <term>crowdsourcing</term>
                    <term>digital research infrastructures and virtual research environments</term>
                    <term>linking and annotation</term>
                    <term>English</term>
                    <term>computer science and informatics</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <div type="div1" rend="DH-Heading">
                <head>
                    <anchor xml:id="id_docs-internal-guid-656f9cbd-7fff-8933-ae49-aa3d9e8d946d"/>
                    <hi rend="color(#000000)bold">Introduction:</hi>
                    <hi rend="color(#000000)"> </hi>
                    <hi rend="color(#000000)bold">Digital Cultural Ecosystems </hi>
                </head>
                <p>The paradigm of ecosystems for digital cultural contents, the so called DCEs (Digital Cultural Ecosystems), can facilitate and foster the process of democratization of knowledge. This process started in the 1990’s with the first applications developed by means of ICT tools applied to Cultural Heritage (Felicori and Guidazzoli, 2003). Open digital frameworks allow the access and the enrichment of cultural digital resources enabling new researches and studies. Moreover, these systems can bring cultural content to different audiences in innovative ways and include citizens in the process of content enrichment (Apollonio et al., 2017) (Paneva-Marinova et al., 2017). 
                    <hi rend="color(#000000)">W</hi>
                    <hi rend="color(#000000)">ith respect to digital Cultural Heritage, citizens should be more than consumers</hi>
                    <hi rend="color(#000000)">
                        <note xml:id="ftn1" place="foot" n="1">
                            <ref target="https://www.vi-mm.eu/wp-content/uploads/2016/12/ViMM-Manifesto-Revised-Final-Revised-19-November.pdf">ViMM Manifesto full version</ref>, ViMM project, retrieved on 3 April 2019.
                        </note>
                    </hi>
                    <hi rend="color(#000000)">. The audience participation should be improved at an active level, implementing user-oriented perspectives. </hi>
                </p>
                <p>
                    <hi rend="color(#000000)">The same principle was at the basis of the I-Media-Cities project, with its aim of involving both researchers and the general public, not only as users of the platform, as mere performers of queries, but, instead, as active contributors by tagging and further enriching the digital contents delivered by the platform. </hi>
                </p>
            </div>
            <div type="div1" rend="DH-Heading">
                <head>
                    <anchor xml:id="id_docs-internal-guid-ea3dc732-7fff-2ad8-0b25-787828ad136d"/>The I-Media-Cities project
                </head>
                <p>
                    <hi rend="color(#000000)">The </hi>
                    <ref target="http://imediacities.eu/">I-Media-Cities</ref>
                    <hi rend="color(#000000)">Horizon 2020 project is the initiative of 9 European Film Libraries, 5 research institutions, 2 technological providers and a specialist of digital business models to share, access to and valorise audiovisual (A/V) content about history and identity of European cities. A huge quantity of fictional and non-fictional AV works (from the end of the 19th century onwards) </hi>
                    <hi rend="color(#000000)">ha</hi>
                    <hi rend="color(#000000)">s</hi>
                    <hi rend="color(#000000)"> been made available </hi>
                    <hi rend="color(#000000)">for research and creative purposes, </hi>describing cities in all aspects, including physical transformation and social dynamics 
                    <anchor xml:id="id_docs-internal-guid-b138c720-7fff-3a10-9246-709c8b6ed7fd"/>
                    <hi rend="color(#000000)">(Table n. 1)</hi>.
                </p>
                <p>The platform, with its dynamic processing pipeline and automatic video analysis tools, enables the enrichment of the moving images meta-data (Fig. 1). By allowing also manual and automatic annotations of the A/V content the platform creates a new digital ecosystem (Caraceni et al., 2017) .</p>
                <p>
                    <figure>
                        <graphic url="Pictures/ee0f4f5bf0954512eb662e8551cdc59c.jpg"/>
                    </figure>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-586d515e-7fff-f3a9-10c6-e744b38f4d18"/>Fig. 1: IMC Movie Processing Pipeline
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-22df397f-7fff-2654-6780-860b1bb6a72b"/>The innovative elements of the I-Media-Cities (IMC) platform are not limited to conveying the audio/visual contents of nine archives to a single collector and access point. Once collected, the A/V material is elaborated with a series of algorithms that automatically enrich it with a good set of meta-data. Nowadays it is a common activity for people to contribute to the enrichment of web contents with various kinds of data, in particular through social media. However, this manual activity is an extremely laborious process; especially when it regards, as in the case of the IMC project, the annotation of videos at shot or single frame level. Automatic meta-data, on the other hand, while not yet achieving the accuracy of a manual enrichment, is able to add a great deal of information. To this end, IMC has integrated a set of tools provided by Fraunhofer for the automatic analysis of video and images. These algorithms provide information on the quality of video, identify camera movements, such as zoom or pan, segment videos in various scenes (shot), and identify and recognize objects and various elements (object detection) that appear in the videos of IMC platform. The object detection tool, thanks to deep learning techniques, is able to identify the presence, for example, of people or means of transport or architectural or urban elements, differentiating between man or woman, adult or child; points out trams or carriages or bicycles; a square or a fountain, and so on (Fig. 2).
                </p>
                <p>
                    <figure>
                        <graphic url="Pictures/899027cbe49aff630f6fcd19c18918f8.png"/>
                    </figure>
                </p>
                <p>Figure 2: Visualisation of the object detection activity. Green detects objects with a confidence higher than 80%, yellow between 50 and 80% and red lower than 50%.</p>
                <p>This is a particularly interesting and complex procedure, which has to manage the analysis of sometimes scarred films digitized several years ago and dating back up to the end of the 19th century.</p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-a8999f54-7fff-e80a-757c-357b78aeb297"/>
                    <figure>
                        <graphic url="Pictures/4d6aec4be3783e19eeb9205468610ea6.png"/>
                    </figure>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-f338a788-7fff-2b65-6e9b-b7ebc5c9aedf"/>Figure 3: IMC platform: video content item page displaying manual and automatic meta-data. Tags are colour coded (automatic, manual from researchers, manual from general users).
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-d725107d-7fff-30d1-7287-58ff4511dc44"/>Since the automatic tools still retain a certain margin of error, a sophisticated online “frame by frame” analysis tool has been set up for the manual correction of inaccuracies on the shot detection, particularly useful on older videos (Fig. 3).
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-f8568485-7fff-b62c-1235-574bb8059042"/>
                    <figure>
                        <graphic url="Pictures/bc7a5b664a96ee4e5144642c7dcbd6b5.png"/>
                    </figure>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-f187d0ac-7fff-139f-baf8-f49e226de040"/>Figure 4: IMC platform: geo-localised search results
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-fe0f445b-7fff-8a16-5d01-71ab9374ddb1"/>The result of this process is the creation of data that are transformed into semantic data, directly understandable by people as well as by computers. The IMC meta-data model summarizes the descriptive meta-data from the archives, and then uploaded directly with the audiovisual materials, based primarily on the 
                    <ref target="http://filmstandards.org/fsc/index.php/Main_Page">CEN EN19507 standard</ref>; and the data generated once the material has been loaded onto the platform. As mentioned, these meta-data can be the result of an automatic or manual enrichment process (Baraldi et al., 2016), modeled using the W3C Web annotation Data Model (WADM).
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-680302ba-7fff-162b-8984-c020109cd875"/>In I-Media-Cities a semantic search engine was developed for processing the requests coming from the user interface and, by analysing all the available meta-data, it provides researchers and citizens with the search results.
                </p>
                <p>The meta-data system includes different types of annotation associated with different details:</p>
                <list type="unordered">
                    <item>original meta-data of the video or image, providing information at the level of the whole content;</item>
                    <item>automatic annotations at the level of a single segment of the video (scene or shot);</item>
                    <item>manual annotations at the level of the single segment of the video (scene or shot), tags and geotag (geolocalised annotation) with which the content has been enriched.</item>
                </list>
                <p>
                    <anchor xml:id="id_docs-internal-guid-58f1759b-7fff-c8ea-9dfd-f4dc9cffe29f"/>
                    <figure>
                        <graphic url="Pictures/009f8b7c11639887f99cbf5cf5282ad6.png"/>
                    </figure>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-b82c2f44-7fff-bf07-81d7-24ee8cde9411"/>Table n.1 - Contents delivered to IMC platform up to February 2019
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-0f157b70-7fff-9f45-76d1-be5d40b457bd"/>In particular, the automatic and manual annotations, being linked to the single segment of the video (scene or shot), allow a collection of scenes belonging to different videos where the same element can be traced: for example the tramways and traffic of the early 1900s (Fig. 5).
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-ced559d1-7fff-37c5-6230-7c090b46cd7c1"/>
                    <figure>
                        <graphic url="Pictures/ad24806e3113adf423b8ee0ad8770df0.png"/>
                    </figure>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-c27ec6b3-7fff-7381-da7d-800e6dd63880"/>Figure 5: IMC platform: search by term
                </p>
                <p>The automatic annotations guarantee that all the audiovisual material is analysed and annotated homogeneously, at least on a common set of aspects, while the manual annotations provide an enrichment of detailed information, also in the form of textual notes, bibliographic references, links to other similar material, both internal and external to I-Media-Cities. At the moment, there are 59,457 manual annotations (with 1,708 different terms); 422,123 automatic annotations (with 78 different terms); 6,411 geotags for 1,091 different places. 
                    <lb/>The performance of the IMC platform rely upon the High Performance Computing resources of Cineca, which allow the use of the most suitable hardware architectures for the different analysis algorithms applied.
                </p>
                <p>In order to adequately present the contents of the research, particular attention is paid to the development of the visual interface, which also allows to perform searches starting directly from the map on which all the AV contents are geolocated, delimiting the selection through a time bar (Fig. 4). In relation to these aspects, a user experience evaluation process is applied, in accordance with the Agile methodology, which pervades the project (Cohen et al., 2004). The Agile methodology implies an iterative approach, with several phases of development: check, correction, check and development (Tab. 2).</p>
                <p>
                    <figure>
                        <graphic url="Pictures/dae5d71544d67d3b4aa4aa46a6ffe9bc.jpg"/>
                    </figure>
                    <anchor xml:id="id_docs-internal-guid-4e166459-7fff-b933-5b38-2984406bb6ed"/>
                </p>
                <p>Table 2: Agile methodology flow chart</p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-386adb0d-7fff-2d4c-cdba-17f6840ac693"/>During the first phase 
                    <hi rend="italic">User and System requirements</hi> were gathered by breaking the project and its requirements down into little parts of user functionalities, called 
                    <hi rend="italic">user stories</hi> and 
                    <hi rend="italic">use cases,</hi> and prioritizing them. Each Film Heritage Institution (FHI) and research partner provided a list of vision items and user stories that were categorised and grouped in 
                    <hi rend="italic">use cases</hi> by the Coordinator of the project and Cineca staff. During this process, the technical partners detailed each 
                    <hi rend="italic">use case</hi> with more technical information, such as technological enablers.
                </p>
                <p>A series of training sessions and a set of “living document” were curated in order to enable this Agile process among researchers and users and make them aware of what was already available and feasible. “Living documents” are continuously updated, following both the development of the project and the state of the art of the different areas on which the project insists.</p>
                <p>The final phase of the project foresees the opening of the platform to the wider public, and not only to researchers. The enrichment of contents can therefore take advantage of crowd-sourcing, obviously subject to a check by researchers and archives on all the information added
                    <note xml:id="ftn2" place="foot" n="2">
                        <p> Among the metadata sent by the archives and associated to each content there is a “right status” value, selected from the following list:</p>
                        <p>- In copyright </p>
                        <p>- EU Orphan Work </p>
                        <p>- In copyright - Educational use permitted </p>
                        <p>- In copyright - Non-commercial use permitted </p>
                        <p>- Public Domain </p>
                        <p>- No Copyright - Contractual Restrictions </p>
                        <p>- No Copyright - Non-Commercial Use Only </p>
                        <p>- No Copyright - Other Known Legal Restrictions </p>
                        <p>- No Copyright - United States </p>
                        <p>- Copyright Undetermined </p>
                        <p>All contents, whatever “right status” they have, can be visualised by everybody, even by the general users group. The archives, depending on the “right status”, can upload low resolution versions with watermarking.</p>
                    </note>. The results of the research can be presented by archives and researchers as virtual exhibitions, set up in a 3D Web environment within the platform itself.
                </p>
            </div>
            <div type="div1" rend="DH-Heading">
                <head>
                    <anchor xml:id="id_docs-internal-guid-0fa87ba2-7fff-86cd-cb03-8d936769444b"/>Conclusions
                </head>
                <p>
                    <hi rend="color(#000000)">I-Media-Cities meets two main requirements of Cultural Heritage sector: gathering information of different types and from different sources and enriching the original information with annotations, automatic or manual ones. Each multimedia content, image or video, is inserted in a process in which the asset is associated with the original </hi>
                    <hi rend="color(#000000)">meta-data</hi>
                    <hi rend="color(#000000)"> belonging to the archive and, then, it is enriched with automatic </hi>
                    <hi rend="color(#000000)">meta-data</hi>
                    <hi rend="color(#000000)"> extracted by different algorithms and further enhanced by manual annotations. In the next phase of the project a particular attention will be devoted in improving the general public user experience (Krug, 2014). The citizens will be able to visualize the public domain content of the archives, search, browse, annotate A/V content and share their discoveries.</hi>
                </p>
                <p>
                    <anchor xml:id="id_docs-internal-guid-71566381-7fff-467a-6c46-3a9a56ef9f28"/>
                    <figure>
                        <graphic url="Pictures/d1281749836ed13d5418a9049e796f11.png"/>
                    </figure>
                </p>
            </div>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend="bold">Apollonio, F. I., Rizzo, F., Bertacchi, S., Dall’Osso, G., Corbelli, A. and Grana, C.</hi> (2017). SACHER: Smart Architecture for Cultural Heritage in Emilia Romagna. In Grana, C. and Baraldi, L. (eds), Digital Libraries and Archives, vol. 733. Cham: Springer International Publishing, pp. 142–56 doi:10.1007/978-3-319-68130-6_12. http://link.springer.com/10.1007/978-3-319-68130-6_12 (accessed 4 March 2019).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Baraldi, L., Grana, C. and Cucchiara, R.</hi> (2016). Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features. Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval - ICMR ’16. New York, New York, USA: ACM Press, pp. 23–29 doi:10.1145/2911996.2912012. http://dl.acm.org/citation.cfm?doid=2911996.2912012 (accessed 4 March 2019).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Caraceni, S., Carpene, M., D’Antonio, M., Fiameni, G., Guidazzoli, A., Imboden, S., Liguori, M. C., et al.</hi> (2017). I-media-cities, a searchable platform on moving images with automatic and manual annotations. 
                        <hi rend="italic">2017 23rd International Conference on Virtual System &amp; Multimedia (VSMM)</hi>. Dublin: IEEE, pp. 1–8 doi:10.1109/VSMM.2017.8346274. https://ieeexplore.ieee.org/document/8346274/ (accessed 4 March 2019).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Cohen, D., Lindvall, M. and Costa, P.</hi> (2004). An Introduction to Agile Methods. 
                        <hi rend="italic">Advances in Computers</hi>, vol. 62. Elsevier, pp. 1–66 doi:10.1016/S0065-2458(03)62001-2. https://linkinghub.elsevier.com/retrieve/pii/S0065245803620012 (accessed 4 March 2019).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Felicori, M. and Guidazzoli, A.</hi> (2003). Experiences in immersive graphics and creation of multimedia database for promoting and enhancing cultural heritage. Roma: IRI Management CRIC-Centro Ricerche su Innovazione e Competitività Eventi Progetti Speciali.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Krug, S.</hi> (2014). 
                        <hi rend="italic">Don’t Make Me Think, Revisited: A Common Sense Approach to Web Usability</hi>. Third edition. Berkeley, Calif.: New Riders.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Paneva-Marinova, D., Pavlov, R. and Kotuzov, N.</hi> (2017). Approach for Analysis and Improved Usage of Digital Cultural Assets for Learning Purposes. 
                        <hi rend="italic">Cybernetics and Information Technologies</hi>, 
                        <hi rend="bold">17</hi>(3): 140–51 doi:10.1515/cait-2017-0035.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
