<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Hypergraph Based Collaborative Film Archive</title>
                <author>
                    <persName>
                        <surname>Biddala</surname>
                        <forename>Sandeep Reddy</forename>
                    </persName>
                    <affiliation>International Institute of Information Technology, Hyderabad (IIIT-H), India</affiliation>
                    <email>biddala.sandeepreddy@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Singh</surname>
                        <forename>Navjyoti</forename>
                    </persName>
                    <affiliation>International Institute of Information Technology, Hyderabad (IIIT-H), India</affiliation>
                    <email>navjyoti@iiit.ac.in</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.9">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>CinemaScope</term>
                    <term>Hypergraph</term>
                    <term>Cinema</term>
                    <term>Data Model</term>
                    <term>Annotation.</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>archives</term>
                    <term>repositories</term>
                    <term>sustainability and preservation</term>
                    <term>audio</term>
                    <term>video</term>
                    <term>multimedia</term>
                    <term>film and cinema studies</term>
                    <term>metadata</term>
                    <term>data modeling and architecture including hypothesis-driven modeling</term>
                    <term>ontologies</term>
                    <term>knowledge representation</term>
                    <term>crowdsourcing</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Digital film archives in addition to preserving cinema should also accommodate and provide for the computation of its cinemas by means of the metadata provided to represent the content in them. The provision for the computation of cinema could lead to various applications like semantic search, automatic genre clustering and classification, special visualization tools, and robust comparisons of movies.</p>
            <p>This paper describes a data model for storing the metadata of a film in an archive based on a discrete theory of cinema that has been in development for the past four years based on Indic theories and other theories of drama and film (Muni, 1996). This theory reduces cinema to a hypergraph composed of tags that assume its full form only with collaborative tagging and extensive visual and textual computing. A system called CinemaScope is also being developed based on this data model which uses HypergraphDB as its database and is being designed and developed to be a semiautomated annotation database system.</p>
            <p>
                <anchor xml:id="h.6pngm6hadp9c"/>Overview of Theory
            </p>
            <p>Cinema is ontologically a three-tier structure:</p>
            <p> 1. Spatio-temporal flow controlled play of light and sound.</p>
            <p> 2. Mere-temporal ‘flow’, which is visually created by a play of 25 fps.</p>
            <p> 3. Trans-temporal content, which is embedded in a ‘flow’.</p>
            <p>Cinema concludes in a thematic unity. Unit of a discrete contiguum of cinema is punctuation. Idea of punctuation is inspired by Leibnizian distinction between ideal point and actual point on the one hand and continuum and discrete contiguum on the other (Leibniz, 2013). Punctuation is a form of an actual point that tells apart two recognizable and non-identical discernible contents. It has a structure &lt;entity1|entity2, relational context&gt; or graphically &lt;node1|node2, edge&gt;. </p>
            <p>There are three classes of punctuations based on the three tiers of ontology of cinema:</p>
            <p> 1. Temporal punctuation. </p>
            <p> 2. Vectorial punctuations between embedded content.</p>
            <p> 3. Mereological punctuations between vectorial contents and the thematic conclusion of cinema.</p>
            <p>This way the form of punctuation constitutes discrete contiguum of cinema, which is computationally reducible to a giant graph that tells truth about cinema as art. </p>
            <p>Ontologically there are categories of discrete content that are embedded in punctuated units of flow like shot and episodes. Some of the categories form the content part of the cinematic narrative (like locale, characters, events, and actions) while the rest (camera work, editing, re-recording) form the expressive part of it (Chatman, 1978). Forces operative on these entities as vector punctuations create kinetics of cinema toward thematic conclusion. The motion in films is through the movement of story or plot from the beginning to the conclusion. Such a motion towards theme is punctuated by points of events and points of considerations. It is through the sequence of events that connection to the theme is established. These events are punctuated with points of considerations. Story can be rendered as sequence of the points of events, and under each event there are several considerations (Dhananjaya, 2004). There are scalars between the sequences of cinema that are merely informative and do not account for the motion of the plot.</p>
            <p>
                <anchor xml:id="h.jm7whto785mz"/>Overview of Data Model
            </p>
            <p>In Hochin (2006) and Hochin and Tatsuo (2000), a data model called Directed Recursive Hypergraph Data Model (DRHM) has been described in which the content of multimedia is reduced to nodes and edges of a hypergraph. In Radev et al. (1999a; 199b), the structural and behavioral aspects of data that form multimedia information systems have been modelled as a graph-based object-oriented model, and a possible data model for film is shown. These approaches suggest that a graph-based data model for cinema based on the discrete theory of cinema proposed in the previous section would best represent it. Punctuation or point &lt;node | node, edge&gt; can be seen as triple of tags. This will make possible computing of the discrete contiguum of cinema as a graph. Also there are complex relations between the various entities of cinema which is best represented by hyperedges.</p>
            <p>The graph data model should accommodate the various ontological entities and their punctuations, the mereological punctuations as well as the vectors between episodes and the considerations between events. The partial data model for the cinematic hypergraph described is represented in the form of Venn diagram as shown in Figure 1.</p>
            <figure>
                <graphic n="1001" width="9.246305555555555cm" height="7.186083333333333cm" url="Pictures/image1.jpg" rend="block"/>
            </figure>
            <p>Figure 1. Venn diagram of cinematic hypergraph—data model.</p>
            <p>Architecture of CinemaScope</p>
            <p>The architecture of the tagging system we propose for CinemaScope is a web-based collaborative one. The annotation schema for the hypergraph data model in theory captures the whole graph. But in practice manual tagging, which is required for the annotating the themes, considerations, meanings, and vectors of cinema, reveals only a sub graph of the original graph. This is because the graph also contains codes and conventions (which are many times dependent on the individual viewer’s perspective) that regulate the narratives and enrich their pure meaning. It is therefore essential that the tagging of the cinema be made social so that most of the graph is captured. The high-level architecture of CinemaScope is given in Figure 2.</p>
            <p>
                <graphic n="1002" width="13.62075cm" height="10.304638888888888cm" url="Pictures/image2.jpg" rend="inline"/>.
            </p>
            <p>Figure 2. Basic architecture of CinemaScope.</p>
            <p>Methodology of CinemaScope</p>
            <p>The database for the system is built on both relational database and graph database (HyperGraphDB). The relational database is used to store all the direct information about the cinema, like its cast, basic plot structure, related files (like script and subtitles), and other relevant information. The graph database is used to graph the tags which have a graphical relation between them. HypergraphDB is a graph-based as well as an object-oriented database with a Java-based API that allows objective modelling of all categories of tags. For example, ontological categories are modelled as follows:</p>
            <p>
                <hi rend="color(000088)">public class</hi>
                <hi rend="color(660066)">OntologicalEntity{</hi>
            </p>
            <p>
                <hi rend="color(000088)">private</hi>
                <hi rend="color(660066)">String</hi> entityName
                <hi rend="color(666600)">;</hi>
                <hi rend="color(880000)">//Entity Name</hi>
            </p>
            <p>
                <hi rend="color(000088)">private</hi>
                <hi rend="color(660066)">Map</hi>
                <hi rend="color(666600)">&lt;</hi>
                <hi rend="color(660066)">String</hi>
                <hi rend="color(666600)">,</hi>
                <hi rend="color(660066)">Object</hi>
                <hi rend="color(666600)">&gt;</hi>attributesNameAndType 
                <hi rend="color(666600)">=</hi>
                <hi rend="color(000088)">new</hi>
                <hi rend="color(660066)">HashMap</hi>
                <hi rend="color(666600)">&lt;</hi>
                <hi rend="color(660066)">String</hi>
                <hi rend="color(666600)">,</hi>
                <hi rend="color(660066)">Object</hi>
                <hi rend="color(666600)">&gt;();</hi>
                <hi rend="color(880000)">//Contains the attributes with their names and values</hi>
            </p>
            <p>
                <hi rend="color(000088)">public void</hi> setEntityName
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">String</hi> entityName
                <hi rend="color(666600)">){</hi>
            </p>
            <p>//Setting The entity Name</p>
            <p>}</p>
            <p>
                <hi rend="color(000088)">public</hi>
                <hi rend="color(660066)">String</hi> getEntityName
                <hi rend="color(666600)">(){</hi>
            </p>
            <p>
                <hi rend="color(000088)">return</hi> entityName;
            </p>
            <p>}</p>
            <p>
                <hi rend="color(000088)">public</hi>
                <hi rend="color(660066)">Object</hi> getAttributeValue
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">String</hi> attribute
                <hi rend="color(666600)">){</hi>
            </p>
            <p>
                <hi rend="color(000088)">return</hi> attributesNameAndType
                <hi rend="color(666600)">.</hi>
                <hi rend="color(000088)">get</hi>
                <hi rend="color(666600)">(</hi>attribute
                <hi rend="color(666600)">);</hi>
            </p>
            <p>}</p>
            <p>
                <hi rend="color(000088)">public void</hi> fillAttribute
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">String</hi> value
                <hi rend="color(666600)">){</hi>
            </p>
            <p>/*Code for filling the attributes*/</p>
            <p>}</p>
            <p>}</p>
            <p>The pre-processing stage involves identification of shots, episodes, and the tagging of basic ontological entities. The pre-processing step cannot be completely automated owing to the limitations of current vision and audio processing algorithms. But this stage of the annotation can be aided with a number of supporting files and techniques. For example, movie screenplays and subtitles that are freely available on the Internet help in giving hints of annotation to the user. Even the processes that are completely automated require manual intervention and editing. There are various shot transition detection methods (Boreczky and Rowe, 1996) along with techniques for identification of camera properties like depth, movement, and angle (Benini et al., 2010). The camera properties identified are dependent on the definitions used in the methods, and they are not completely accurate. The tagging system should provide options for manually editing and supervising the shots and camera properties.</p>
            <p>Film scripts and subtitles aid in the tagging of ontological entities by supplying hints. The script is first aligned with the subtitles for time stamps based on the work of Ronfard and Theung (2003). A basic version of this has been implemented based on the work of Larissa Munishkina et al. (2013). An example hint given by the system for the film 
                <hi rend="italic">Indiana Jones and the Raiders of the Lost Ark</hi> is as follows:
            </p>
            <p>E.g. Scene #1 (0:00–2:15)</p>
            <p> Characters: Indiana Jones, Baranaca</p>
            <p> Locale: High Jungle, Peru</p>
            <p> Things: Gun, Idol</p>
            <p> Events: Shooting, Running</p>
            <p>The Collaborative or Social tagging follows the pre-processing step, and it helps in the creation of tags and links in cinema. The user is given the provision of defining the relations, which is made possible by the use of HyperGraphDB. Hertzum et al. (2002) suggests that collaboration of film archives should facilitate different archives in identifying a common ground on which to base a collaborator and in acknowledging the distinctiveness of each archive. The architecture of CinemaScope allows identifying a common ground, in terms of pre-processing tags as well as distinctiveness, by allowing the users of different background to provide interpretations and considerations of the story in terms of vectors, relations, and links (each with a different label).</p>
            <p>The various relations added by the user and the tags obtained from the pre-processing stage build up the graph, which could later be used for various applications like searching. For example, the following code snippet answers semantic queries like ‘Find all the camera properties used when character is holding the whip’:</p>
            <p>
                <hi rend="color(660066)">HyperGraph</hi> graph 
                <hi rend="color(666600)">=</hi>
                <hi rend="color(000088)">new</hi>
                <hi rend="color(660066)">HyperGraph</hi>
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">HyperGraphLocation</hi>
                <hi rend="color(666600)">);</hi>
            </p>
            <p>
                <hi rend="color(660066)">List</hi>
                <hi rend="color(666600)">&lt;</hi>
                <hi rend="color(660066)">CameraTemporalRelations</hi>
                <hi rend="color(666600)">&gt;</hi> cameras 
                <hi rend="color(666600)">=</hi> hg
                <hi rend="color(666600)">.</hi>getAll
                <hi rend="color(666600)">(</hi>hg
                <hi rend="color(666600)">.</hi>type
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">CameraTemporalRelations</hi>
                <hi rend="color(666600)">));</hi>
            </p>
            <p>
                <hi rend="color(660066)">List</hi>
                <hi rend="color(666600)">&lt;</hi>
                <hi rend="color(660066)">CharacterThingTemporalRelations</hi>
                <hi rend="color(666600)">&gt;</hi> CTR 
                <hi rend="color(666600)">=</hi> hg
                <hi rend="color(666600)">.</hi>getAll
                <hi rend="color(666600)">(</hi>hg
                <hi rend="color(666600)">.</hi>
                <hi rend="color(000088)">and</hi>
                <hi rend="color(666600)">(</hi>hg
                <hi rend="color(666600)">.</hi>type
                <hi rend="color(666600)">(</hi>
                <hi rend="color(660066)">CharacterThingTemporalRelations</hi>
                <hi rend="color(666600)">.</hi>
                <hi rend="color(000088)">class</hi>
                <hi rend="color(666600)">),</hi> hg
                <hi rend="color(666600)">.</hi>eq
                <hi rend="color(666600)">(</hi>
                <hi rend="color(008800)">"spatial_relation"</hi>
                <hi rend="color(666600)">,</hi>
                <hi rend="color(008800)">"hold"</hi>
                <hi rend="color(666600)">),</hi> hg
                <hi rend="color(666600)">.</hi>eq
                <hi rend="color(666600)">(</hi>
                <hi rend="color(008800)">"thing_name"</hi>
                <hi rend="color(666600)">,</hi>
                <hi rend="color(008800)">"whip"</hi>
                <hi rend="color(666600)">)));</hi>
            </p>
            <p>
                <anchor xml:id="h.u2e32glmo8qr"/>Summary
            </p>
            <p>The paper describes the discrete theory of cinema and explains the hypergraph data model of cinema. The paper also describes CinemaScope, a semi-automated web-based collaborative film archive based on the hypergraph data model for annotating the film. This project is different from other movie annotation projects (ANSWER, 2009; Jewell et al., 2005); Lombardo and Damiano, 2010), which fail to capture the flow of cinema and make the computational representation of cinema in a database very descriptive and computationally infeasible.</p>
            <figure>
                <graphic n="1003" width="16.51cm" height="9.278055555555556cm" url="Pictures/image3.png" rend="block"/>
            </figure>
            <p>Figure 3. Prototype of CinemaScope annotation system.</p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend="bold">ANSWER Annual Report.</hi> (2009). http://cordis.europa.eu/fp7/ict/content-knowledge/docs/answer-annual-report-2009.pdf.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Benini, S., Canini, L. and Leonardi, R.</hi> (2010). Estimating Cinematographic Scene Depth in Movie Shots. Multimedia and Expo (ICME), 2010 IEEE International Conference.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Boreczky, J. S. and Rowe, L. A.</hi> (1996). Comparison of Video Shot Boundary Detection. 
                        <hi rend="italic">Journal of Electronic Imaging,</hi>
                        <hi rend="bold">5</hi>(2) (April): 122–28.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Chatman, S.</hi> (1978). 
                        <hi rend="italic">Story and Discourse: Narrative Structure in Fiction and Film</hi>. Cornell University Press, Ithaca, NY.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Dhananjaya.</hi> (1100). Dasarupaka, Dr. Keshavrao Musalgaunkar. Hindi Dasarupaka, 2004.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Hertzum, M., Pejtersen, A. M., Cleal, B. and Albrechtsen, H.</hi> (2002). An Analysis of Collaboration in Three Film Archives: A Case for Collaboratories. 
                        <hi rend="italic">CoLIS4: Proceedings of the Fourth International Conference on Conceptions of Library and Information Science.</hi> Libraries Unlimited, pp. 69–83.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Hochin, T.</hi> (2006). Graph-Based Data Model for the Content Representation of Multimedia Data. Knowledge-Based Intelligent Information and Engineering Systems. 
                        <hi rend="italic">Lecture Notes in Computer Science,</hi>
                        <hi rend="bold">4252</hi> (1182–90).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Hochin, T. and Tatsuo T.</hi> (2000). A Directed Recursive Hypergraph Data Model for Representing the Contents of Multimedia Data. 
                        <hi rend="italic">Memoirs of the Faculty of Engineering, Fukui University, </hi>
                        <hi rend="bold">48</hi>: 343–60.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Jewell, M., Lawrence, K., Tuffield, M., Prugel-Bennett, A., Millard, D., Nixon, M. and Shadbolt, N.</hi> (2005). OntoMedia: An Ontology for the Representation of Heterogeneous Media. 
                        <hi rend="italic">Proceedings of SIGIR Workshop on Multimedia Information Retrieval.</hi> ACM.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Leibniz, G. W.</hi> (2013). 
                        <hi rend="italic">The Labyrinth of the Continuum: Writings on the Continuum Problem 1672–1686</hi>. Translated, edited, and with an introduction by Richard T. W. Arthur. Yale University Press, New Haven, CT.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Lombardo, V. and Damiano, R.</hi> (2010). Narrative Annotation and Editing of Video. Interactive Storytelling. 
                        <hi rend="italic">Lecture Notes in Computer Science,</hi>
                        <hi rend="bold">6432</hi>, 62–73.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Muni, B.</hi> (1996 [300 BC]). 
                        <hi rend="italic">Nāṭya śāstra</hi>. With Abhinavagupta’s Commentary; with Hindi Commentary by Dwivedi Parasnath. Sampurnanand Sanskrit Mahavidyalaya, Varanasi.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Munishkina, L., Parrish, J. and Walker, M. A.</hi> (2013). Fully Automatic Interactive Story Design from Film Scripts. Interactive Storytelling. 
                        <hi rend="italic">Lecture Notes in Computer Science,</hi>
                        <hi rend="bold">8230</hi>: 229–32.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Radev, I., Pissinou, N. and Makki, K.</hi> (1999a). Film Video Modeling. 
                        <hi rend="italic">Proceedings of IEEE Workshop on Knowledge and Data Engineering Exchange, KDEX 99</hi>, Chicago, IL, November 1999.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Radev, I., Pissinou, N., Makki, K. and Park, E. K. </hi>(1999b). Graph-Based Object-Oriented Approach for Structural and Behavioral Representation of Multimedia Data. 
                        <hi rend="italic">Proceedings of the Eighth International Conference on Information and Knowledge Management</hi>, Kansas City, MO, 2–6 November 1999, pp. 522–30.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Ronfard, R. and Thuong, T. T.</hi> (2003). A Framework for Aligning and Indexing Movies with Their Script. 
                        <hi rend="italic">Proceedings of ICME</hi>, Baltimore, MD, 6–9 July 2003.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
